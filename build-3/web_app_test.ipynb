{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-04T16:30:30.681967400Z",
     "start_time": "2024-12-04T16:30:30.673146100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import stat\n",
    "import cv2\n",
    "import dlib\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from tensorflow.keras.models import Sequential, Model, save_model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.14\n"
     ]
    }
   ],
   "source": [
    "print(mp.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T22:37:01.356320200Z",
     "start_time": "2024-12-03T22:37:01.348321300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_dir = r'D:\\PycharmProjects\\pro_dis_2\\collected_test_data'\n",
    "extracted_frames_dir = os.path.join(base_dir, \"extracted_frames\")\n",
    "processed_frames_dir = os.path.join(base_dir, \"processed_frames\")\n",
    "cropped_frames_dir = os.path.join(base_dir, \"cropped_frames\")\n",
    "combined_image_path = os.path.join(base_dir, \"combined_frames.png\")\n",
    "\n",
    "os.chmod(base_dir, stat.S_IWUSR | stat.S_IRUSR | stat.S_IXUSR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T16:30:33.192231100Z",
     "start_time": "2024-12-04T16:30:33.172970600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**frame extraction functions**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def extract_frame(frame, output_dir, frame_index):\n",
    "    frame_filename = os.path.join(output_dir, f\"{frame_index:02d}.png\")  # Format frame index with leading zeros\n",
    "    cv2.imwrite(frame_filename, frame)\n",
    "    return frame_filename\n",
    "\n",
    "def extract_frames_with_priority_deletion(video_path, output_dir, target_frames=60):\n",
    "    # Create base directory for storing extracted frames\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    # Handle case when the video has fewer than the target number of frames\n",
    "    if total_frames < target_frames:\n",
    "        for i in range(total_frames):\n",
    "            extract_frame(frames[i], output_dir, i + 1)  # Start frame index from 1\n",
    "\n",
    "        # Copy the last frame to fill the deficit until the target number is reached\n",
    "        last_frame = frames[-1]\n",
    "        for i in range(total_frames, target_frames):\n",
    "            extract_frame(last_frame, output_dir, i + 1)\n",
    "\n",
    "        print(f\"Copied last frame to fill the deficit for {video_path}.\")\n",
    "        cap.release()\n",
    "        return\n",
    "\n",
    "    # Handle case when the video has more than the target number of frames\n",
    "    if total_frames > target_frames:\n",
    "        frames_to_delete = total_frames - target_frames\n",
    "        delete_from_end = int(frames_to_delete * 0.9)  # 50% of frames to delete from the end\n",
    "        delete_from_start = frames_to_delete - delete_from_end  # 20% from the start\n",
    "\n",
    "        # Retain the middle portion after deleting the required frames\n",
    "        frames = frames[delete_from_start:total_frames - delete_from_end]\n",
    "\n",
    "    # Extract frames after deletion logic or for target-sized videos\n",
    "    for i in range(len(frames)):\n",
    "        extract_frame(frames[i], output_dir, i + 1)\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {len(frames)} frames saved at: {output_dir}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T16:30:34.450969700Z",
     "start_time": "2024-12-04T16:30:34.437152800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**extracted image processing**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from albumentations import Compose, RandomBrightnessContrast\n",
    "\n",
    "# Function for sharpening the image\n",
    "def sharpen_image(image):\n",
    "    \"\"\"Apply sharpening to the image.\"\"\"\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    return cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "# Albumentations pipeline for brightness and contrast adjustment\n",
    "def get_brightness_contrast_augmentation():\n",
    "    \"\"\"\n",
    "    Returns a pipeline to adjust brightness and contrast without probability.\n",
    "    \"\"\"\n",
    "    return Compose([\n",
    "        RandomBrightnessContrast(\n",
    "            brightness_limit=(0.05, 0.05),  # Fixed Brightness adjustment range: +5%\n",
    "            contrast_limit=(0.05, 0.05),    # Fixed Contrast adjustment range: +5%\n",
    "            p=1.0                         # Always apply\n",
    "        )\n",
    "    ])\n",
    "\n",
    "# Unified function to apply sharpening and brightness/contrast adjustment\n",
    "def process_extracted_frames(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Applies sharpening and brightness/contrast adjustment to all images in a folder.\n",
    "    :param input_folder: Path to the folder containing images.\n",
    "    :param output_folder: Path to save the processed images.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Get the brightness/contrast augmentation pipeline\n",
    "    augmentation_pipeline = get_brightness_contrast_augmentation()\n",
    "\n",
    "    for file_name in sorted(os.listdir(input_folder)):\n",
    "        if file_name.endswith(\".png\"):\n",
    "            input_path = os.path.join(input_folder, file_name)\n",
    "            output_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "            # Read the image\n",
    "            image = cv2.imread(input_path)\n",
    "            if image is None:\n",
    "                print(f\"Error reading image: {input_path}\")\n",
    "                continue\n",
    "\n",
    "            # Step 1: Apply sharpening\n",
    "            sharpened_image = sharpen_image(image)\n",
    "\n",
    "            # Step 2: Apply brightness and contrast adjustment\n",
    "            augmented = augmentation_pipeline(image=sharpened_image)\n",
    "            final_image = augmented['image']\n",
    "\n",
    "            # Save the processed image\n",
    "            cv2.imwrite(output_path, final_image)\n",
    "\n",
    "    print(f\"Processed extracted frames and saved at: {output_folder}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T16:54:26.170248700Z",
     "start_time": "2024-12-04T16:54:26.156609500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**frames cropping function and parameters**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# Load the detector and predictor (dlib models)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"D:/PycharmProjects/pro_dis_2/models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Mouth crop dimensions\n",
    "LIP_HEIGHT = 80\n",
    "LIP_WIDTH = 112\n",
    "\n",
    "def crop_frame(frame_file, path, output_path):\n",
    "    \"\"\"\n",
    "    Save a single frame as a .png image in the specified directory.\n",
    "    Ensures complete image save before moving to the next frame.\n",
    "    \"\"\"\n",
    "    frame_path = os.path.join(path, frame_file)\n",
    "    try:\n",
    "        # Load the frame\n",
    "        frame = cv2.imread(frame_path)\n",
    "        if frame is None:\n",
    "            print(f\"Warning: Could not read frame {frame_path}. Skipping.\")\n",
    "            return False\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the grayscale image\n",
    "        faces = detector(gray)\n",
    "\n",
    "        if not faces:\n",
    "            return False\n",
    "\n",
    "        # Only process if a face is detected\n",
    "        for face in faces:\n",
    "            landmarks = predictor(gray, face)\n",
    "\n",
    "            # Extract the mouth region by iterating over the landmarks (48 to 67)\n",
    "            mouth_points = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(48, 68)]\n",
    "            mouth_points_np = np.array(mouth_points)\n",
    "\n",
    "            # Find the bounding rectangle around the mouth points\n",
    "            x, y, w, h = cv2.boundingRect(mouth_points_np)\n",
    "\n",
    "            # Calculate padding to fit the target dimensions\n",
    "            width_diff = LIP_WIDTH - w\n",
    "            height_diff = LIP_HEIGHT - h\n",
    "            pad_left = max(width_diff // 2, 0)\n",
    "            pad_right = max(width_diff - pad_left, 0)\n",
    "            pad_top = max(height_diff // 2, 0)\n",
    "            pad_bottom = max(height_diff - pad_top, 0)\n",
    "\n",
    "            # Adjust padding to ensure it doesn’t exceed image boundaries\n",
    "            pad_left = min(pad_left, x)\n",
    "            pad_right = min(pad_right, frame.shape[1] - (x + w))\n",
    "            pad_top = min(pad_top, y)\n",
    "            pad_bottom = min(pad_bottom, frame.shape[0] - (y + h))\n",
    "\n",
    "            # Crop and resize the mouth region\n",
    "            lip_frame = frame[y - pad_top:y + h + pad_bottom, x - pad_left:x + w + pad_right]\n",
    "            lip_frame = cv2.resize(lip_frame, (LIP_WIDTH, LIP_HEIGHT))\n",
    "\n",
    "            # Save the cropped mouth region to the output directory\n",
    "            output_frame_path = os.path.join(output_path, frame_file)\n",
    "            cv2.imwrite(output_frame_path, lip_frame)\n",
    "\n",
    "            return True  # Exit after processing the first detected face\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing frame {frame_path}: {e}\")\n",
    "        return False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T16:30:42.330955200Z",
     "start_time": "2024-12-04T16:30:40.563162Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**combining cropped frames**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def combine_images(input_path, output_path, output_filename=\"combined_frames.png\"):\n",
    "    # List all frame files and sort them to maintain order\n",
    "    frame_files = [f for f in os.listdir(input_path) if f.endswith('.png')]\n",
    "    frame_files.sort()  # Ensure the frames are in order\n",
    "\n",
    "    # Check that there are exactly 60 frames\n",
    "    if len(frame_files) != 60:\n",
    "        print(f\"Warning: {input_path} does not contain exactly 60 frames. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Load the first image to get dimensions\n",
    "    first_image = cv2.imread(os.path.join(input_path, frame_files[0]))\n",
    "    if first_image is None:\n",
    "        print(f\"Error: Could not read {frame_files[0]}.\")\n",
    "        return\n",
    "\n",
    "    img_height, img_width, channels = first_image.shape\n",
    "\n",
    "    # Create an empty array for the combined image (10 rows × 6 columns)\n",
    "    combined_image = np.zeros((img_height * 10, img_width * 6, channels), dtype=np.uint8)\n",
    "\n",
    "    # Place each frame into the correct position in the combined image\n",
    "    for idx, frame_file in enumerate(frame_files):\n",
    "        img = cv2.imread(os.path.join(input_path, frame_file))\n",
    "        if img is None:\n",
    "            print(f\"Error: Could not read {frame_file}.\")\n",
    "            continue\n",
    "\n",
    "        row = idx // 6\n",
    "        col = idx % 6\n",
    "\n",
    "        # Place the image in the combined image array\n",
    "        combined_image[row * img_height:(row + 1) * img_height, col * img_width:(col + 1) * img_width] = img\n",
    "\n",
    "    # Save the combined image\n",
    "    cv2.imwrite(os.path.join(output_path, output_filename), combined_image)\n",
    "\n",
    "    print(f\"\\nCombined image saved at: {os.path.join(output_path, output_filename)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T16:30:45.638213200Z",
     "start_time": "2024-12-04T16:30:45.618046700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**models loading**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the .h5 file\n",
    "lip_model = load_model(r'C:\\Users\\avikd\\OneDrive - Sheffield Hallam University\\Desktop\\Project & Dissertation\\8. Saved models\\lip detection\\model2811_361_21_d130_GOOD.h5')\n",
    "# lip_model.summary()\n",
    "\n",
    "# Load the saved class labels\n",
    "with open(r'C:\\Users\\avikd\\OneDrive - Sheffield Hallam University\\Desktop\\Project & Dissertation\\8. Saved models\\class_labels_cl10.json', 'r') as f:\n",
    "    class_labels = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T16:30:49.594202400Z",
     "start_time": "2024-12-04T16:30:48.422636Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# class_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T22:38:39.580717600Z",
     "start_time": "2024-12-03T22:38:39.557912200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# Load T5 model and tokenizer for sentence generation\n",
    "load_directory = r\"C:\\Users\\avikd\\OneDrive - Sheffield Hallam University\\Desktop\\Project & Dissertation\\8. Saved models\\text generation\\t5_fine_tuned_local\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(load_directory)\n",
    "txt_model = T5ForConditionalGeneration.from_pretrained(load_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T16:30:52.829711300Z",
     "start_time": "2024-12-04T16:30:51.861558400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**integration with web app**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def is_open_hand(hand_landmarks):\n",
    "    \"\"\" Check if all fingers are extended (open hand) \"\"\"\n",
    "    for finger_tip, finger_pip in [\n",
    "        (mp_hands.HandLandmark.INDEX_FINGER_TIP, mp_hands.HandLandmark.INDEX_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.MIDDLE_FINGER_TIP, mp_hands.HandLandmark.MIDDLE_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.RING_FINGER_TIP, mp_hands.HandLandmark.RING_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.PINKY_TIP, mp_hands.HandLandmark.PINKY_PIP)\n",
    "    ]:\n",
    "        if hand_landmarks.landmark[finger_tip].y > hand_landmarks.landmark[finger_pip].y:\n",
    "            return False  # A finger is not extended\n",
    "    return True  # All fingers are extended\n",
    "\n",
    "def is_closed_fist(hand_landmarks):\n",
    "    \"\"\" Check if all fingers are folded (closed fist) \"\"\"\n",
    "    for finger_tip, finger_pip in [\n",
    "        (mp_hands.HandLandmark.INDEX_FINGER_TIP, mp_hands.HandLandmark.INDEX_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.MIDDLE_FINGER_TIP, mp_hands.HandLandmark.MIDDLE_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.RING_FINGER_TIP, mp_hands.HandLandmark.RING_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.PINKY_TIP, mp_hands.HandLandmark.PINKY_PIP)\n",
    "    ]:\n",
    "        if hand_landmarks.landmark[finger_tip].y < hand_landmarks.landmark[finger_pip].y:\n",
    "            return False  # A finger is extended\n",
    "    return True  # All fingers are folded"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T16:30:55.097620400Z",
     "start_time": "2024-12-04T16:30:55.089483700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001B[31m\u001B[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001B[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001B[33mPress CTRL+C to quit\u001B[0m\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:35] \"GET /get-status HTTP/1.1\" 200 -\n",
      "Exception in thread Thread-1891:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\urllib3\\connection.py\", line 199, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\", line 495, in _make_request\n",
      "    conn.request(\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\urllib3\\connection.py\", line 441, in request\n",
      "    self.endheaders()\n",
      "  File \"C:\\Users\\avikd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py\", line 1278, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Users\\avikd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py\", line 1038, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"C:\\Users\\avikd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py\", line 976, in send\n",
      "    self.connect()\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\urllib3\\connection.py\", line 279, in connect\n",
      "    self.sock = self._new_conn()\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\urllib3\\connection.py\", line 214, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001ABA0F52140>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ABA0F52140>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\avikd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\avikd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 1378, in run\n",
      "    self.function(*self.args, **self.kwargs)\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\flask_ngrok.py\", line 70, in start_ngrok\n",
      "    ngrok_address = _run_ngrok()\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\flask_ngrok.py\", line 35, in _run_ngrok\n",
      "    tunnel_url = requests.get(localhost_url).text  # Get the tunnel information\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001ABA0F52140>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:41] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:42] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:43] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:44] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:45] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:46] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:47] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:48] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:49] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:50] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:51] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:52] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:53] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:54] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:55] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:56] \"GET /get-status HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 60 frames saved at: D:\\PycharmProjects\\pro_dis_2\\collected_test_data\\extracted_frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:57] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:58] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:54:59] \"GET /get-status HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed extracted frames and saved at: D:\\PycharmProjects\\pro_dis_2\\collected_test_data\\processed_frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:00] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:01] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:02] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:03] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:04] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:05] \"GET /get-status HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined image saved at: D:\\PycharmProjects\\pro_dis_2\\collected_test_data\\combined_frames.png\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:06] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:07] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:08] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:09] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:10] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:11] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:12] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:13] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:14] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:15] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:16] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:17] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:18] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:19] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:20] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:21] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:22] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:23] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:24] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:25] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:26] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:27] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:28] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:29] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:30] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:31] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:32] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:33] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:34] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:35] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:36] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:37] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:38] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:39] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:40] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:41] \"GET /get-status HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 60 frames saved at: D:\\PycharmProjects\\pro_dis_2\\collected_test_data\\extracted_frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:42] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:43] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:44] \"GET /get-status HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed extracted frames and saved at: D:\\PycharmProjects\\pro_dis_2\\collected_test_data\\processed_frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:45] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:46] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:47] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:48] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:49] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:50] \"GET /get-status HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined image saved at: D:\\PycharmProjects\\pro_dis_2\\collected_test_data\\combined_frames.png\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 50ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:51] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:52] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:53] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:54] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:55] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:56] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:57] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:58] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:55:59] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:00] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:01] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:02] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:03] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:04] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:05] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:06] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:07] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:08] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:09] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:10] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:11] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:12] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:13] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:14] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:15] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:16] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:17] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:18] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:19] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:20] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:21] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:22] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:23] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:24] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:25] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:26] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:27] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:28] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:29] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:30] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:31] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:32] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:33] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:34] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:35] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:36] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:37] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:38] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:39] \"GET /get-status HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Dec/2024 16:56:40] \"GET /get-status HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, jsonify\n",
    "from flask_ngrok import run_with_ngrok\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "run_with_ngrok(app)\n",
    "\n",
    "# Directories\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "os.makedirs(extracted_frames_dir, exist_ok=True)\n",
    "os.makedirs(cropped_frames_dir, exist_ok=True)\n",
    "\n",
    "# Initialize MediaPipe Hands and Drawing modules\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize variables\n",
    "recording = False\n",
    "video_writer = None\n",
    "executor = ThreadPoolExecutor(max_workers=1)\n",
    "status_messages = []  # List to store real-time status messages\n",
    "status_lock = threading.Lock()  # Lock for thread-safe updates\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    \"\"\"Serve the main web interface.\"\"\"\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/get-status', methods=['GET'])\n",
    "def get_status():\n",
    "    \"\"\"Serve the latest status updates to the web app.\"\"\"\n",
    "    with status_lock:\n",
    "        return jsonify({\"status\": status_messages})\n",
    "\n",
    "def add_status_message(message):\n",
    "    \"\"\"Add a status message safely.\"\"\"\n",
    "    with status_lock:\n",
    "        status_messages.append(message)\n",
    "        # Limit the size of status_messages to avoid overflow\n",
    "        if len(status_messages) > 50:\n",
    "            status_messages.pop(0)\n",
    "\n",
    "def process_video_in_background(video_path, extracted_frames_dir, processed_frames_dir, cropped_frames_dir, combined_frames_dir, target_frames=60):\n",
    "    \"\"\"Background process to handle frame extraction, cropping, and prediction.\"\"\"\n",
    "    try:\n",
    "        add_status_message(\"Processing video...\")\n",
    "\n",
    "        # Step 1: Extract frames\n",
    "        extract_frames_with_priority_deletion(video_path, extracted_frames_dir, target_frames)\n",
    "        add_status_message(f\"Extracted frames saved at: {extracted_frames_dir}\")\n",
    "\n",
    "        # Step 2: Process extracted frames\n",
    "        add_status_message(\"Processing extracted frames with sharpening and brightness/contrast adjustments...\")\n",
    "        process_extracted_frames(extracted_frames_dir, processed_frames_dir)\n",
    "        add_status_message(f\"Processed frames saved at: {processed_frames_dir}\")\n",
    "\n",
    "        # Step 3: Crop processed frames\n",
    "        frame_files = [f for f in os.listdir(processed_frames_dir) if f.endswith('.png')]\n",
    "        cropped_count = sum(crop_frame(frame_file, processed_frames_dir, cropped_frames_dir) for frame_file in frame_files)\n",
    "        add_status_message(f\"Finished cropping all frames. Images cropped -> {cropped_count}\")\n",
    "\n",
    "        # Step 4: Combine cropped frames\n",
    "        combine_images(cropped_frames_dir, combined_frames_dir)\n",
    "        add_status_message(f\"Combined image saved at: {combined_frames_dir}\")\n",
    "\n",
    "        # Step 5: Predict word and generate a sentence\n",
    "        if os.path.exists(combined_image_path):\n",
    "            img = Image.open(combined_image_path).resize((224, 224), Image.LANCZOS)\n",
    "            img_array = np.array(img) / 255.0\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "            predictions = lip_model.predict(img_array)\n",
    "            predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "            predicted_word = class_labels[str(predicted_class_index)]\n",
    "\n",
    "            input_text = f\"Generate a sentence for {predicted_word}:\"\n",
    "            input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "            outputs = txt_model.generate(input_ids, max_length=20, do_sample=True, top_k=50, top_p=0.9, temperature=0.9, num_return_sequences=1)\n",
    "            generated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            add_status_message(f\"Predicted Word: <b><i>{predicted_word}</i></b>\")\n",
    "            add_status_message(f\"Generated Sentence: <b><i>{generated_sentence}</i></b>\")\n",
    "            add_status_message(\"\\n\")\n",
    "        else:\n",
    "            add_status_message(\"Error: Combined image not found.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        add_status_message(f\"Error in processing: {e}\")\n",
    "\n",
    "def open_video_feed():\n",
    "    \"\"\"Open the video feed and detect gestures.\"\"\"\n",
    "    global recording, video_writer\n",
    "\n",
    "    # cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Initialize video capture\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "    # Set the resolution to the maximum supported by your camera\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)  # Set width (e.g., 1280 for 720p)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)  # Set height (e.g., 720 for 720p)\n",
    "\n",
    "    with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            image_height, image_width, _ = frame.shape\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.flip(image, 1)\n",
    "            results = hands.process(image)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                        mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2)\n",
    "                    )\n",
    "\n",
    "                    if is_open_hand(hand_landmarks):\n",
    "                        cv2.putText(image, \"Open Hand Detected\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                        if not recording:\n",
    "                            recording = True\n",
    "                            video_name = \"recording.mp4\"\n",
    "                            output_file = os.path.join(base_dir, video_name)\n",
    "                            video_writer = cv2.VideoWriter(\n",
    "                                output_file,\n",
    "                                cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                                30,\n",
    "                                (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "                            )\n",
    "                            add_status_message(\"Recording started...\")\n",
    "\n",
    "                    elif is_closed_fist(hand_landmarks):\n",
    "                        cv2.putText(image, \"Closed Fist Detected\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                        if recording:\n",
    "                            recording = False\n",
    "                            video_writer.release()\n",
    "                            add_status_message(f\"Recording stopped and saved at: {output_file}\")\n",
    "                            executor.submit(process_video_in_background, output_file, extracted_frames_dir, processed_frames_dir, cropped_frames_dir, base_dir)\n",
    "\n",
    "            if recording and video_writer is not None:\n",
    "                video_writer.write(frame)\n",
    "\n",
    "            cv2.imshow('Hand Gesture Recognition', image)\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    threading.Thread(target=open_video_feed, daemon=True).start()\n",
    "    app.run()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T16:56:41.350212700Z",
     "start_time": "2024-12-04T16:54:31.714802100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cap.release()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
