{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-22T10:48:00.036593400Z",
     "start_time": "2024-11-22T10:48:00.004499300Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from albumentations import (\n",
    "    HorizontalFlip, ShiftScaleRotate, RandomBrightnessContrast, GaussianBlur, GaussNoise, HueSaturationValue\n",
    ")\n",
    "from scipy.ndimage import gaussian_filter, map_coordinates\n",
    "\n",
    "# Load the image\n",
    "image_path = r\"D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\test_image.png\"  # Replace with your image path\n",
    "image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)  # Load image with alpha channel if present\n",
    "\n",
    "# Separate RGB and Alpha channel (if alpha channel exists)\n",
    "if image.shape[-1] == 4:  # RGBA image\n",
    "    rgb_image = image[:, :, :3]\n",
    "    alpha_channel = image[:, :, 3]\n",
    "else:\n",
    "    rgb_image = image\n",
    "    alpha_channel = None\n",
    "\n",
    "# Create output directory\n",
    "output_dir = r\"D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Rotation** (can change clockwise or anticlockwise rotation with different angles under 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rotate_image(image, angle=5):\n",
    "    h, w = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    return cv2.warpAffine(image, matrix, (w, h))\n",
    "\n",
    "rotated_image = rotate_image(rgb_image, angle=5)\n",
    "cv2.imwrite(f\"{output_dir}/rotated.png\", rotated_image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T10:55:20.204660500Z",
     "start_time": "2024-11-21T10:55:20.158784700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Scaling** (can use different scale multiplier)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scale_image(image, scale=1.2):\n",
    "    h, w = image.shape[:2]\n",
    "    return cv2.resize(image, (int(w * scale), int(h * scale)))\n",
    "\n",
    "scaled_image = scale_image(rgb_image, scale=1.2)\n",
    "cv2.imwrite(f\"{output_dir}/scaled.png\", scaled_image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T10:22:38.550974200Z",
     "start_time": "2024-11-21T10:22:38.490418800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Translation** (need different type of shift values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate_image(image, x_shift=10, y_shift=10):\n",
    "    h, w = image.shape[:2]\n",
    "    matrix = np.float32([[1, 0, x_shift], [0, 1, y_shift]])\n",
    "    return cv2.warpAffine(image, matrix, (w, h))\n",
    "\n",
    "translated_image = translate_image(rgb_image, x_shift=10, y_shift=10)\n",
    "cv2.imwrite(f\"{output_dir}/translated.png\", translated_image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T10:28:39.185816500Z",
     "start_time": "2024-11-21T10:28:39.145646900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Flipping** (random horizontal flipping)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipped_image = cv2.flip(rgb_image, 1)  # Horizontal flip\n",
    "cv2.imwrite(f\"{output_dir}/flipped.png\", flipped_image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T10:33:32.935183500Z",
     "start_time": "2024-11-21T10:33:32.876101600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Gaussian Noise** (can apply the variations of noise values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_gaussian_noise(image, mean=0, var=10):\n",
    "    sigma = var ** 0.5\n",
    "    gauss = np.random.normal(mean, sigma, image.shape).astype('uint8')\n",
    "    noisy_image = cv2.add(image, gauss)\n",
    "    return noisy_image\n",
    "\n",
    "noisy_image = add_gaussian_noise(rgb_image)\n",
    "cv2.imwrite(f\"{output_dir}/gaussian_noise.png\", noisy_image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T10:34:27.497904800Z",
     "start_time": "2024-11-21T10:34:27.374341900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Brightness/Contrast Adjustments** (can apply either brightness or contrast or both with different values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adjust_brightness_contrast(image, brightness=30, contrast=30):\n",
    "    return cv2.convertScaleAbs(image, alpha=1 + contrast / 100.0, beta=brightness)\n",
    "\n",
    "bright_contrast_image = adjust_brightness_contrast(rgb_image, brightness=30, contrast=30)\n",
    "cv2.imwrite(f\"{output_dir}/brightness_contrast.png\", bright_contrast_image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T10:35:34.363914800Z",
     "start_time": "2024-11-21T10:35:34.317673500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Blurring/Sharpening** (can apply either blur or sharpen with different values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def blur_image(image, blur_value=5):\n",
    "    return cv2.GaussianBlur(image, (blur_value, blur_value), 0)\n",
    "\n",
    "blurred_image = blur_image(rgb_image, blur_value=5)\n",
    "cv2.imwrite(f\"{output_dir}/blurred.png\", blurred_image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T10:36:49.301500200Z",
     "start_time": "2024-11-21T10:36:49.256407Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Color Jittering** (can use more noticeable color jittering calibration)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def color_jitter(image, hue_shift=10, sat_shift=20, val_shift=20):\n",
    "    jitter = HueSaturationValue(hue_shift_limit=hue_shift, sat_shift_limit=sat_shift, val_shift_limit=val_shift, p=1.0)\n",
    "    augmented = jitter(image=image)\n",
    "    return augmented['image']\n",
    "\n",
    "color_jittered_image = color_jitter(rgb_image)\n",
    "cv2.imwrite(f\"{output_dir}/color_jittered.png\", color_jittered_image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T10:38:40.651505700Z",
     "start_time": "2024-11-21T10:38:40.579495600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Affine Transformations** (use as minimum as possible)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def affine_transform(image):\n",
    "    h, w = image.shape[:2]\n",
    "    src_points = np.float32([[0, 0], [w - 1, 0], [0, h - 1]])\n",
    "    dst_points = np.float32([\n",
    "        [w * 0.1, h * 0.1],\n",
    "        [w * 0.9, h * 0.2],\n",
    "        [w * 0.2, h * 0.8]\n",
    "    ])\n",
    "    matrix = cv2.getAffineTransform(src_points, dst_points)\n",
    "    return cv2.warpAffine(image, matrix, (w, h))\n",
    "\n",
    "affine_image = affine_transform(rgb_image)\n",
    "cv2.imwrite(f\"{output_dir}/affine.png\", affine_image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T10:44:11.757027200Z",
     "start_time": "2024-11-21T10:44:11.721110100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Occlusion Simulation** (can change size of the Occlusion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_occlusion(image, max_size=40):\n",
    "    \"\"\"\n",
    "    Add black occlusions to random areas of the image.\n",
    "    \"\"\"\n",
    "    occluded_image = image.copy()  # Create a copy to avoid modifying the original\n",
    "    h, w = occluded_image.shape[:2]\n",
    "    for _ in range(np.random.randint(1, 3)):\n",
    "        x1, y1 = np.random.randint(0, w), np.random.randint(0, h)\n",
    "        x2, y2 = min(x1 + max_size, w), min(y1 + max_size, h)\n",
    "        occluded_image[y1:y2, x1:x2] = 0  # Black occlusion\n",
    "    return occluded_image\n",
    "\n",
    "occluded_image = add_occlusion(rgb_image)\n",
    "cv2.imwrite(f\"{output_dir}/occluded_fixed.png\", occluded_image)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T10:49:47.850360Z",
     "start_time": "2024-11-22T10:49:47.807111200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Specular Highlights** (can change size of the Highlights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_highlight(image, intensity=0.5, size=30):\n",
    "    h, w = image.shape[:2]\n",
    "    overlay = image.copy()\n",
    "    x, y = np.random.randint(0, w), np.random.randint(0, h)\n",
    "    cv2.circle(overlay, (x, y), size, (255, 255, 255), -1)\n",
    "    return cv2.addWeighted(overlay, intensity, image, 1 - intensity, 0)\n",
    "\n",
    "highlighted_image = add_highlight(rgb_image)\n",
    "cv2.imwrite(f\"{output_dir}/highlighted.png\", highlighted_image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T10:55:00.077717700Z",
     "start_time": "2024-11-21T10:55:00.026082Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pixel-Level Augmentation** (can increase or decrease the pixel dropout)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pixel_dropout(image, drop_percent=0.02):\n",
    "    mask = np.random.choice([0, 1], size=image.shape[:2], p=[drop_percent, 1 - drop_percent])\n",
    "    dropout_image = image.copy()\n",
    "    dropout_image[mask == 0] = 0  # Black out pixels\n",
    "    return dropout_image\n",
    "\n",
    "dropout_image = pixel_dropout(rgb_image, drop_percent=0.02)\n",
    "cv2.imwrite(f\"{output_dir}/pixel_dropout.png\", dropout_image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T10:54:24.920506800Z",
     "start_time": "2024-11-21T10:54:24.842374600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**pre-trained lip movement model** (use as minimum as possible)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load DLIB's pre-trained model for face detection and landmarks\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(r\"D:\\PycharmProjects\\pro_dis_2\\models\\shape_predictor_68_face_landmarks.dat\")  # Download from DLIB's GitHub\n",
    "\n",
    "def get_lip_landmarks(image):\n",
    "    \"\"\"\n",
    "    Detect lip landmarks using DLIB.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "    if len(faces) == 0:\n",
    "        print(\"No face detected.\")\n",
    "        return None\n",
    "\n",
    "    # Get landmarks for the first face detected\n",
    "    shape = predictor(gray, faces[0])\n",
    "    landmarks = np.array([[p.x, p.y] for p in shape.parts()])\n",
    "\n",
    "    # Extract lip landmarks (indices 48-67 for lips in DLIB's model)\n",
    "    lip_landmarks = landmarks[48:68]\n",
    "    return lip_landmarks\n",
    "\n",
    "def exaggerate_lip_movement(image, lip_landmarks, scale=1.5):\n",
    "    \"\"\"\n",
    "    Exaggerate lip movements by scaling the lip landmarks.\n",
    "    \"\"\"\n",
    "    center = np.mean(lip_landmarks, axis=0).astype(np.int32)\n",
    "    exaggerated_landmarks = (lip_landmarks - center) * scale + center\n",
    "\n",
    "    # Create a copy of the image to draw on\n",
    "    output = image.copy()\n",
    "    for point in exaggerated_landmarks:\n",
    "        point = tuple(map(int, point))  # Ensure point is an integer tuple\n",
    "        cv2.circle(output, point, 2, (0, 255, 0), -1)\n",
    "    return output\n",
    "\n",
    "# Load the image\n",
    "image_path = r\"D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\test_image.png\"  # Replace with your image path\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Get lip landmarks and exaggerate\n",
    "lip_landmarks = get_lip_landmarks(image)\n",
    "if lip_landmarks is not None:\n",
    "    exaggerated_image = exaggerate_lip_movement(image, lip_landmarks, scale=1.5)\n",
    "    cv2.imwrite(r\"D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\exaggerated_lips.png\", exaggerated_image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T13:28:07.130830300Z",
     "start_time": "2024-11-21T13:28:05.661350400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. sharpen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied sharpening on 01.png.\n",
      "Applied sharpening on 02.png.\n",
      "Applied sharpening on 03.png.\n",
      "Applied sharpening on 04.png.\n",
      "Applied sharpening on 05.png.\n",
      "Applied sharpening on 06.png.\n",
      "Applied sharpening on 07.png.\n",
      "Applied sharpening on 08.png.\n",
      "Applied sharpening on 09.png.\n",
      "Applied sharpening on 10.png.\n",
      "Applied sharpening on 11.png.\n",
      "Applied sharpening on 12.png.\n",
      "Applied sharpening on 13.png.\n",
      "Applied sharpening on 14.png.\n",
      "Applied sharpening on 15.png.\n",
      "Applied sharpening on 16.png.\n",
      "Applied sharpening on 17.png.\n",
      "Applied sharpening on 18.png.\n",
      "Applied sharpening on 19.png.\n",
      "Applied sharpening on 20.png.\n",
      "Applied sharpening on 21.png.\n",
      "Applied sharpening on 22.png.\n",
      "Applied sharpening on 23.png.\n",
      "Applied sharpening on 24.png.\n",
      "Applied sharpening on 25.png.\n",
      "Applied sharpening on 26.png.\n",
      "Applied sharpening on 27.png.\n",
      "Applied sharpening on 28.png.\n",
      "Applied sharpening on 29.png.\n",
      "Applied sharpening on 30.png.\n",
      "Applied sharpening on 31.png.\n",
      "Applied sharpening on 32.png.\n",
      "Applied sharpening on 33.png.\n",
      "Applied sharpening on 34.png.\n",
      "Applied sharpening on 35.png.\n",
      "Applied sharpening on 36.png.\n",
      "Applied sharpening on 37.png.\n",
      "Applied sharpening on 38.png.\n",
      "Applied sharpening on 39.png.\n",
      "Applied sharpening on 40.png.\n",
      "Applied sharpening on 41.png.\n",
      "Applied sharpening on 42.png.\n",
      "Applied sharpening on 43.png.\n",
      "Applied sharpening on 44.png.\n",
      "Applied sharpening on 45.png.\n",
      "Applied sharpening on 46.png.\n",
      "Applied sharpening on 47.png.\n",
      "Applied sharpening on 48.png.\n",
      "Applied sharpening on 49.png.\n",
      "Applied sharpening on 50.png.\n",
      "Applied sharpening on 51.png.\n",
      "Applied sharpening on 52.png.\n",
      "Applied sharpening on 53.png.\n",
      "Applied sharpening on 54.png.\n",
      "Applied sharpening on 55.png.\n",
      "Applied sharpening on 56.png.\n",
      "Applied sharpening on 57.png.\n",
      "Applied sharpening on 58.png.\n",
      "Applied sharpening on 59.png.\n",
      "Applied sharpening on 60.png.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def sharpen_image(image):\n",
    "    \"\"\"Apply sharpening to the image.\"\"\"\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    return cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "def apply_sharpening(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Apply sharpening to all images in a folder.\n",
    "    :param input_folder: Path to the folder containing images.\n",
    "    :param output_folder: Path to save the sharpened images.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for file_name in sorted(os.listdir(input_folder)):\n",
    "        if file_name.endswith(\".png\"):\n",
    "            input_path = os.path.join(input_folder, file_name)\n",
    "            output_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "            # Read the image\n",
    "            image = cv2.imread(input_path)\n",
    "\n",
    "            # Apply sharpening\n",
    "            sharpened_image = sharpen_image(image)\n",
    "\n",
    "            # Save the sharpened image\n",
    "            cv2.imwrite(output_path, sharpened_image)\n",
    "            print(f\"Applied sharpening on {file_name}.\")\n",
    "\n",
    "# Example usage\n",
    "input_folder = r\"D:\\PycharmProjects\\pro_dis_2\\collected_test_data\\extracted_frames\"\n",
    "output_folder = r\"D:\\PycharmProjects\\pro_dis_2\\collected_test_data\\output_folder_sharp\"\n",
    "apply_sharpening(input_folder, output_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T15:24:06.270470900Z",
     "start_time": "2024-12-04T15:24:03.146704400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Brightness/Contrast"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed images with brightness and contrast adjustment saved in: D:\\PycharmProjects\\pro_dis_2\\collected_test_data\\output_frames_with_brightness_contrast\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from albumentations import Compose, RandomBrightnessContrast\n",
    "\n",
    "# Albumentations augmentation pipeline for brightness and contrast adjustment\n",
    "def get_brightness_contrast_augmentation():\n",
    "    \"\"\"\n",
    "    Returns a pipeline to adjust brightness and contrast without probability.\n",
    "    \"\"\"\n",
    "    return Compose([\n",
    "        RandomBrightnessContrast(\n",
    "            brightness_limit=(0.1, 0.1),  # Brightness adjustment range: ±30%\n",
    "            contrast_limit=(0.1, 0.1),   # Contrast adjustment range: ±30%\n",
    "            p=1.0                        # Always apply\n",
    "        )\n",
    "    ])\n",
    "\n",
    "# Function to process images with brightness/contrast adjustment\n",
    "def process_images_with_brightness_contrast_albumentations(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Processes all images in the input folder and saves them to the output folder\n",
    "    after applying brightness and contrast adjustment using Albumentations.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Get augmentation pipeline\n",
    "    augmentation_pipeline = get_brightness_contrast_augmentation()\n",
    "\n",
    "    for file_name in sorted(os.listdir(input_folder)):\n",
    "        if file_name.endswith(\".png\"):\n",
    "            input_path = os.path.join(input_folder, file_name)\n",
    "            output_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "            # Read the image\n",
    "            image = cv2.imread(input_path)\n",
    "            if image is None:\n",
    "                print(f\"Error reading image: {input_path}\")\n",
    "                continue\n",
    "\n",
    "            # Apply brightness and contrast adjustment\n",
    "            augmented = augmentation_pipeline(image=image)\n",
    "            augmented_image = augmented['image']\n",
    "\n",
    "            # Save the augmented image\n",
    "            cv2.imwrite(output_path, augmented_image)\n",
    "\n",
    "    print(f\"Processed images with brightness and contrast adjustment saved in: {output_folder}\")\n",
    "\n",
    "# Define input and output directories\n",
    "input_folder = r\"D:\\PycharmProjects\\pro_dis_2\\collected_test_data\\output_folder_sharp\"\n",
    "output_folder = r\"D:\\PycharmProjects\\pro_dis_2\\collected_test_data\\output_frames_with_brightness_contrast\"\n",
    "\n",
    "# Run the function\n",
    "process_images_with_brightness_contrast_albumentations(input_folder, output_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T15:49:52.155087200Z",
     "start_time": "2024-12-04T15:49:48.990232700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. sharpen + 10% brightness and contrast"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing of images...\n",
      "Processed 01.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 02.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 03.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 04.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 05.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 06.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 07.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 08.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 09.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 10.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 11.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 12.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 13.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 14.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 15.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 16.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 17.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 18.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 19.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 20.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 21.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 22.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 23.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 24.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 25.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 26.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 27.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 28.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 29.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 30.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 31.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 32.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 33.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 34.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 35.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 36.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 37.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 38.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 39.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 40.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 41.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 42.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 43.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 44.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 45.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 46.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 47.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 48.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 49.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 50.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 51.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 52.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 53.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 54.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 55.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 56.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 57.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 58.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 59.png with sharpening and brightness/contrast adjustment.\n",
      "Processed 60.png with sharpening and brightness/contrast adjustment.\n",
      "Processing completed. All images are saved in the output folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from albumentations import Compose, RandomBrightnessContrast\n",
    "\n",
    "# Function for sharpening the image\n",
    "def sharpen_image(image):\n",
    "    \"\"\"Apply sharpening to the image.\"\"\"\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    return cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "# Albumentations pipeline for brightness and contrast adjustment\n",
    "def get_brightness_contrast_augmentation():\n",
    "    \"\"\"\n",
    "    Returns a pipeline to adjust brightness and contrast without probability.\n",
    "    \"\"\"\n",
    "    return Compose([\n",
    "        RandomBrightnessContrast(\n",
    "            brightness_limit=(0.05, 0.05),  # Fixed Brightness adjustment range: +5%\n",
    "            contrast_limit=(0.05, 0.05),    # Fixed Contrast adjustment range: +5%\n",
    "            p=1.0                         # Always apply\n",
    "        )\n",
    "    ])\n",
    "\n",
    "# Unified function to apply sharpening and brightness/contrast adjustment\n",
    "def process_images(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Applies sharpening and brightness/contrast adjustment to all images in a folder.\n",
    "    :param input_folder: Path to the folder containing images.\n",
    "    :param output_folder: Path to save the processed images.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Get the brightness/contrast augmentation pipeline\n",
    "    augmentation_pipeline = get_brightness_contrast_augmentation()\n",
    "\n",
    "    for file_name in sorted(os.listdir(input_folder)):\n",
    "        if file_name.endswith(\".png\"):\n",
    "            input_path = os.path.join(input_folder, file_name)\n",
    "            output_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "            # Read the image\n",
    "            image = cv2.imread(input_path)\n",
    "            if image is None:\n",
    "                print(f\"Error reading image: {input_path}\")\n",
    "                continue\n",
    "\n",
    "            # Step 1: Apply sharpening\n",
    "            sharpened_image = sharpen_image(image)\n",
    "\n",
    "            # Step 2: Apply brightness and contrast adjustment\n",
    "            augmented = augmentation_pipeline(image=sharpened_image)\n",
    "            final_image = augmented['image']\n",
    "\n",
    "            # Save the processed image\n",
    "            cv2.imwrite(output_path, final_image)\n",
    "            print(f\"Processed {file_name} with sharpening and brightness/contrast adjustment.\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Define input and output directories\n",
    "    input_folder = r\"D:\\PycharmProjects\\pro_dis_2\\collected_test_data\\extracted_frames\"\n",
    "    output_folder = r\"D:\\PycharmProjects\\pro_dis_2\\collected_test_data\\final_frames_with_sharpening_brightness_contrast\"\n",
    "\n",
    "    # Apply combined processing\n",
    "    print(\"Starting processing of images...\")\n",
    "    process_images(input_folder, output_folder)\n",
    "    print(\"Processing completed. All images are saved in the output folder.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T15:55:36.943266500Z",
     "start_time": "2024-12-04T15:55:33.653131800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
