{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-09T00:02:26.554053100Z",
     "start_time": "2025-01-09T00:02:18.711049400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\albumentations\\__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.21). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from albumentations import Compose, RandomBrightnessContrast\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from tensorflow.keras.models import Sequential, Model, save_model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.14\n"
     ]
    }
   ],
   "source": [
    "print(mp.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-09T00:02:26.563816100Z",
     "start_time": "2025-01-09T00:02:26.554053100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_dir = r'D:\\PycharmProjects\\source project files\\collected_test_data'\n",
    "extracted_frames_dir = r'D:\\PycharmProjects\\source project files\\collected_test_data\\extracted_frames_one'\n",
    "processed_frames_dir = r'D:\\PycharmProjects\\source project files\\collected_test_data\\processed_frames_two'\n",
    "cropped_frames_dir = r'D:\\PycharmProjects\\source project files\\collected_test_data\\cropped_frames_three'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-09T00:03:12.933158200Z",
     "start_time": "2025-01-09T00:03:12.923681400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**frame extraction functions**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def extract_frame(frame, output_dir, frame_index):\n",
    "    frame_filename = os.path.join(output_dir, f\"{frame_index:02d}.png\")  # Format frame index with leading zeros\n",
    "    cv2.imwrite(frame_filename, frame)\n",
    "    return frame_filename\n",
    "\n",
    "def extract_frames_with_priority_deletion(video_path, output_dir, target_frames=60):\n",
    "    # Create base directory for storing extracted frames\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    # Handle case when the video has fewer than the target number of frames\n",
    "    if total_frames < target_frames:\n",
    "        for i in range(total_frames):\n",
    "            extract_frame(frames[i], output_dir, i + 1)  # Start frame index from 1\n",
    "\n",
    "        # Copy the last frame to fill the deficit until the target number is reached\n",
    "        last_frame = frames[-1]\n",
    "        for i in range(total_frames, target_frames):\n",
    "            extract_frame(last_frame, output_dir, i + 1)\n",
    "\n",
    "        print(f\"Copied last frame to fill the deficit for {video_path}.\")\n",
    "        cap.release()\n",
    "        return\n",
    "\n",
    "    # Handle case when the video has more than the target number of frames\n",
    "    if total_frames > target_frames:\n",
    "        frames_to_delete = total_frames - target_frames\n",
    "        delete_from_end = int(frames_to_delete * 0.9)  # 50% of frames to delete from the end\n",
    "        delete_from_start = frames_to_delete - delete_from_end  # 20% from the start\n",
    "\n",
    "        # Retain the middle portion after deleting the required frames\n",
    "        frames = frames[delete_from_start:total_frames - delete_from_end]\n",
    "\n",
    "    # Extract frames after deletion logic or for target-sized videos\n",
    "    for i in range(len(frames)):\n",
    "        extract_frame(frames[i], output_dir, i + 1)\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {len(frames)} frames saved at: {output_dir}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-09T00:03:14.113785100Z",
     "start_time": "2025-01-09T00:03:14.097456800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**extracted image processing**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Function for sharpening the image\n",
    "def sharpen_image(image):\n",
    "    \"\"\"Apply sharpening to the image.\"\"\"\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    return cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "# Albumentations pipeline for brightness and contrast adjustment\n",
    "def get_brightness_contrast_augmentation():\n",
    "    \"\"\"\n",
    "    Returns a pipeline to adjust brightness and contrast without probability.\n",
    "    \"\"\"\n",
    "    return Compose([\n",
    "        RandomBrightnessContrast(\n",
    "            brightness_limit=(0.05, 0.05),  # Fixed Brightness adjustment range: +5%\n",
    "            contrast_limit=(0.05, 0.05),    # Fixed Contrast adjustment range: +5%\n",
    "            p=1.0                         # Always apply\n",
    "        )\n",
    "    ])\n",
    "\n",
    "# Unified function to apply sharpening and brightness/contrast adjustment\n",
    "def process_extracted_frames(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Applies sharpening and brightness/contrast adjustment to all images in a folder.\n",
    "    :param input_folder: Path to the folder containing images.\n",
    "    :param output_folder: Path to save the processed images.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Get the brightness/contrast augmentation pipeline\n",
    "    augmentation_pipeline = get_brightness_contrast_augmentation()\n",
    "\n",
    "    for file_name in sorted(os.listdir(input_folder)):\n",
    "        if file_name.endswith(\".png\"):\n",
    "            input_path = os.path.join(input_folder, file_name)\n",
    "            output_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "            # Read the image\n",
    "            image = cv2.imread(input_path)\n",
    "            if image is None:\n",
    "                print(f\"Error reading image: {input_path}\")\n",
    "                continue\n",
    "\n",
    "            # Step 1: Apply sharpening\n",
    "            sharpened_image = sharpen_image(image)\n",
    "\n",
    "            # Step 2: Apply brightness and contrast adjustment\n",
    "            augmented = augmentation_pipeline(image=sharpened_image)\n",
    "            final_image = augmented['image']\n",
    "\n",
    "            # Save the processed image\n",
    "            cv2.imwrite(output_path, final_image)\n",
    "\n",
    "    print(f\"Processed extracted frames and saved at: {output_folder}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-09T00:03:17.378899900Z",
     "start_time": "2025-01-09T00:03:17.356243Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**frames cropping function and parameters**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Load the detector and predictor (dlib models)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(r\"D:\\PycharmProjects\\source project files\\models\\shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Mouth crop dimensions\n",
    "LIP_HEIGHT = 80\n",
    "LIP_WIDTH = 112\n",
    "\n",
    "def process_frame(frame_file, path, output_path):\n",
    "    \"\"\"\n",
    "    Save a single frame as a .png image in the specified directory.\n",
    "    Ensures complete image save before moving to the next frame.\n",
    "    \"\"\"\n",
    "    frame_path = os.path.join(path, frame_file)\n",
    "    try:\n",
    "        # Load the frame\n",
    "        frame = cv2.imread(frame_path)\n",
    "        if frame is None:\n",
    "            print(f\"Warning: Could not read frame {frame_path}. Skipping.\")\n",
    "            return False\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the grayscale image\n",
    "        faces = detector(gray)\n",
    "\n",
    "        if not faces:\n",
    "            return False\n",
    "\n",
    "        # Only process if a face is detected\n",
    "        for face in faces:\n",
    "            landmarks = predictor(gray, face)\n",
    "\n",
    "            # Extract the mouth region by iterating over the landmarks (48 to 67)\n",
    "            mouth_points = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(48, 68)]\n",
    "            mouth_points_np = np.array(mouth_points)\n",
    "\n",
    "            # Find the bounding rectangle around the mouth points\n",
    "            x, y, w, h = cv2.boundingRect(mouth_points_np)\n",
    "\n",
    "            # Calculate padding to fit the target dimensions\n",
    "            width_diff = LIP_WIDTH - w\n",
    "            height_diff = LIP_HEIGHT - h\n",
    "            pad_left = max(width_diff // 2, 0)\n",
    "            pad_right = max(width_diff - pad_left, 0)\n",
    "            pad_top = max(height_diff // 2, 0)\n",
    "            pad_bottom = max(height_diff - pad_top, 0)\n",
    "\n",
    "            # Adjust padding to ensure it doesn’t exceed image boundaries\n",
    "            pad_left = min(pad_left, x)\n",
    "            pad_right = min(pad_right, frame.shape[1] - (x + w))\n",
    "            pad_top = min(pad_top, y)\n",
    "            pad_bottom = min(pad_bottom, frame.shape[0] - (y + h))\n",
    "\n",
    "            # Crop and resize the mouth region\n",
    "            lip_frame = frame[y - pad_top:y + h + pad_bottom, x - pad_left:x + w + pad_right]\n",
    "            lip_frame = cv2.resize(lip_frame, (LIP_WIDTH, LIP_HEIGHT))\n",
    "\n",
    "            # Save the cropped mouth region to the output directory\n",
    "            output_frame_path = os.path.join(output_path, frame_file)\n",
    "            cv2.imwrite(output_frame_path, lip_frame)\n",
    "\n",
    "            return True  # Exit after processing the first detected face\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing frame {frame_path}: {e}\")\n",
    "        return False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-09T00:04:23.738245200Z",
     "start_time": "2025-01-09T00:04:22.121558600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**combining cropped frames**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def combine_images(input_path, output_path, output_filename=\"combined_frames.png\"):\n",
    "    # List all frame files and sort them to maintain order\n",
    "    frame_files = [f for f in os.listdir(input_path) if f.endswith('.png')]\n",
    "    frame_files.sort()  # Ensure the frames are in order\n",
    "\n",
    "    # Check that there are exactly 60 frames\n",
    "    if len(frame_files) != 60:\n",
    "        print(f\"Warning: {input_path} does not contain exactly 60 frames. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Load the first image to get dimensions\n",
    "    first_image = cv2.imread(os.path.join(input_path, frame_files[0]))\n",
    "    if first_image is None:\n",
    "        print(f\"Error: Could not read {frame_files[0]}.\")\n",
    "        return\n",
    "\n",
    "    img_height, img_width, channels = first_image.shape\n",
    "\n",
    "    # Create an empty array for the combined image (10 rows × 6 columns)\n",
    "    combined_image = np.zeros((img_height * 10, img_width * 6, channels), dtype=np.uint8)\n",
    "\n",
    "    # Place each frame into the correct position in the combined image\n",
    "    for idx, frame_file in enumerate(frame_files):\n",
    "        img = cv2.imread(os.path.join(input_path, frame_file))\n",
    "        if img is None:\n",
    "            print(f\"Error: Could not read {frame_file}.\")\n",
    "            continue\n",
    "\n",
    "        row = idx // 6\n",
    "        col = idx % 6\n",
    "\n",
    "        # Place the image in the combined image array\n",
    "        combined_image[row * img_height:(row + 1) * img_height, col * img_width:(col + 1) * img_width] = img\n",
    "\n",
    "    # Save the combined image\n",
    "    cv2.imwrite(os.path.join(output_path, output_filename), combined_image)\n",
    "\n",
    "    print(f\"\\nCombined image saved at: {os.path.join(output_path, output_filename)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-09T00:04:29.572360600Z",
     "start_time": "2025-01-09T00:04:29.556998800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**models loading**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": "\u001B[1mModel: \"sequential_2\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d_4 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m32\u001B[0m)   │           \u001B[38;5;34m896\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_4 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m112\u001B[0m, \u001B[38;5;34m112\u001B[0m, \u001B[38;5;34m32\u001B[0m)   │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_5 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m112\u001B[0m, \u001B[38;5;34m112\u001B[0m, \u001B[38;5;34m64\u001B[0m)   │        \u001B[38;5;34m18,496\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_5 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m56\u001B[0m, \u001B[38;5;34m56\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_2 (\u001B[38;5;33mFlatten\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m200704\u001B[0m)         │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │    \u001B[38;5;34m51,380,480\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │        \u001B[38;5;34m32,896\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m10\u001B[0m)             │         \u001B[38;5;34m1,290\u001B[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200704</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">51,380,480</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m51,434,060\u001B[0m (196.21 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,434,060</span> (196.21 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m51,434,058\u001B[0m (196.21 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,434,058</span> (196.21 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Optimizer params: \u001B[0m\u001B[38;5;34m2\u001B[0m (12.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model from the .h5 file\n",
    "lip_model = load_model(r'D:\\PycharmProjects\\source project files\\Saved models\\lip detection\\model2811_361_21_d130_GOOD.h5')\n",
    "lip_model.summary()\n",
    "\n",
    "# Load the saved class labels\n",
    "with open(r'D:\\PycharmProjects\\source project files\\Saved models\\class_labels_cl10.json', 'r') as f:\n",
    "    class_labels = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-09T00:05:58.515912800Z",
     "start_time": "2025-01-09T00:05:57.989024Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# class_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-09T00:06:01.525493600Z",
     "start_time": "2025-01-09T00:06:01.513967500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Load T5 model and tokenizer for sentence generation\n",
    "load_directory = r\"D:\\PycharmProjects\\source project files\\Saved models\\text generation\\t5_fine_tuned_local\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(load_directory)\n",
    "txt_model = T5ForConditionalGeneration.from_pretrained(load_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-09T00:06:25.605426700Z",
     "start_time": "2025-01-09T00:06:24.586710600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**integration with T5 model**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 116\u001B[0m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m mp_hands\u001B[38;5;241m.\u001B[39mHands(min_detection_confidence\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.8\u001B[39m, min_tracking_confidence\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m hands:\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m cap\u001B[38;5;241m.\u001B[39misOpened():\n\u001B[1;32m--> 116\u001B[0m         ret, frame \u001B[38;5;241m=\u001B[39m \u001B[43mcap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    117\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ret:\n\u001B[0;32m    118\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize MediaPipe Hands and Drawing modules\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "# Set the resolution to the maximum supported by your camera\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)  # Set width (e.g., 1280 for 720p)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)  # Set height (e.g., 720 for 720p)\n",
    "\n",
    "# Directories checking\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "os.makedirs(extracted_frames_dir, exist_ok=True)\n",
    "os.makedirs(cropped_frames_dir, exist_ok=True)\n",
    "combined_image_path = os.path.join(base_dir, \"combined_frames.png\")\n",
    "\n",
    "# Variables to handle recording\n",
    "recording = False\n",
    "video_writer = None\n",
    "executor = ThreadPoolExecutor(max_workers=1)  # Thread pool for frame extraction\n",
    "\n",
    "# Set video codec and file format\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30  # Default to 30 if unable to read fps\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------\n",
    "# while updating the code just paste the previous block which includes is_open_hand(), is_closed_fist() and process_video_in_background()\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------\n",
    "def is_open_hand(hand_landmarks):\n",
    "    \"\"\" Check if all fingers are extended (open hand) \"\"\"\n",
    "    for finger_tip, finger_pip in [\n",
    "        (mp_hands.HandLandmark.INDEX_FINGER_TIP, mp_hands.HandLandmark.INDEX_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.MIDDLE_FINGER_TIP, mp_hands.HandLandmark.MIDDLE_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.RING_FINGER_TIP, mp_hands.HandLandmark.RING_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.PINKY_TIP, mp_hands.HandLandmark.PINKY_PIP)\n",
    "    ]:\n",
    "        if hand_landmarks.landmark[finger_tip].y > hand_landmarks.landmark[finger_pip].y:\n",
    "            return False  # A finger is not extended\n",
    "    return True  # All fingers are extended\n",
    "\n",
    "def is_closed_fist(hand_landmarks):\n",
    "    \"\"\" Check if all fingers are folded (closed fist) \"\"\"\n",
    "    for finger_tip, finger_pip in [\n",
    "        (mp_hands.HandLandmark.INDEX_FINGER_TIP, mp_hands.HandLandmark.INDEX_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.MIDDLE_FINGER_TIP, mp_hands.HandLandmark.MIDDLE_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.RING_FINGER_TIP, mp_hands.HandLandmark.RING_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.PINKY_TIP, mp_hands.HandLandmark.PINKY_PIP)\n",
    "    ]:\n",
    "        if hand_landmarks.landmark[finger_tip].y < hand_landmarks.landmark[finger_pip].y:\n",
    "            return False  # A finger is extended\n",
    "    return True  # All fingers are folded\n",
    "\n",
    "def process_video_in_background(video_path, extracted_frames_dir, processed_frames_dir, cropped_frames_dir, combined_frames_dir, target_frames=60):\n",
    "    \"\"\"Run frame extraction and cropping in a separate thread\"\"\"\n",
    "    print(f\"\\nProcessing video...\")\n",
    "\n",
    "    # Call the frame extraction function\n",
    "    extract_frames_with_priority_deletion(video_path, extracted_frames_dir, target_frames)\n",
    "\n",
    "    # Process extracted frames\n",
    "    print(f\"\\nSharpening and adjusting brightness/contrast of extracted frames...\")\n",
    "    process_extracted_frames(extracted_frames_dir, processed_frames_dir)\n",
    "\n",
    "    # Perform cropping on the processed extracted frames\n",
    "    frame_files = [f for f in os.listdir(extracted_frames_dir) if f.endswith('.png')]\n",
    "    cropped_count = 0\n",
    "\n",
    "    for frame_file in frame_files:\n",
    "        if process_frame(frame_file, extracted_frames_dir, cropped_frames_dir):\n",
    "            cropped_count += 1\n",
    "\n",
    "    print(f\"\\nFinished processing all frames. Images cropped -> {cropped_count}\")\n",
    "\n",
    "    # Combine cropped frames into a single image\n",
    "    combine_images(cropped_frames_dir, combined_frames_dir)\n",
    "    print(f\"All frames combined and saved in {combined_frames_dir}.\")\n",
    "\n",
    "    # Perform word prediction from the combined image\n",
    "    if os.path.exists(combined_image_path):\n",
    "        img = Image.open(combined_image_path).resize((224, 224), Image.LANCZOS)\n",
    "        img_array = np.array(img) / 255.0  # Normalize pixel values\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "        predictions = lip_model.predict(img_array)\n",
    "        predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "        predicted_word = class_labels[str(predicted_class_index)]\n",
    "        print(f\"\\nPredicted word: {predicted_word}\")\n",
    "        print(predictions)\n",
    "\n",
    "        # Generate a sentence from the predicted word\n",
    "        input_text = f\"Generate a sentence for {predicted_word}:\"\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        outputs = txt_model.generate(\n",
    "            input_ids,\n",
    "            max_length=20,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            temperature=0.9,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "\n",
    "        generated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"\\nGenerated Sentence: {generated_sentence}\")\n",
    "\n",
    "    print('----------------------------------------------------------------------')\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------\n",
    "# while updating the code just paste the previous block which includes is_open_hand(), is_closed_fist() and process_video_in_background()\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image_height, image_width, _ = frame.shape\n",
    "\n",
    "        # Convert BGR to RGB and flip for mirror effect\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.flip(image, 1)\n",
    "        image.flags.writeable = False  # Set to False for faster processing\n",
    "\n",
    "        # Process the image and find hands\n",
    "        results = hands.process(image)\n",
    "        image.flags.writeable = True  # Set to True for drawing\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Check if hands are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for num, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "                # Gesture recognition logic\n",
    "                if is_open_hand(hand_landmarks):\n",
    "                    cv2.putText(image, \"Open Hand Detected\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    if not recording:\n",
    "                        recording = True\n",
    "                        video_name = \"recording.mp4\"  # Change this if needed for unique names\n",
    "                        output_file = os.path.join(base_dir, video_name)\n",
    "                        video_writer = cv2.VideoWriter(\n",
    "                            output_file,\n",
    "                            cv2.VideoWriter_fourcc(*'mp4v'),  # Codec for .mp4 format\n",
    "                            fps,\n",
    "                            (frame_width, frame_height)\n",
    "                        )\n",
    "                        if not video_writer.isOpened():\n",
    "                            print(\"Error: Video writer failed to open.\")\n",
    "                            recording = False\n",
    "                        else:\n",
    "                            print(\"\\nRecording started...\")\n",
    "\n",
    "                elif is_closed_fist(hand_landmarks):\n",
    "                    cv2.putText(image, \"Closed Fist Detected\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                    if recording:\n",
    "                        recording = False\n",
    "                        if video_writer is not None:\n",
    "                            video_writer.release()\n",
    "                            print(f\"Recording stopped and saved at: {output_file}\")\n",
    "                        video_writer = None  # Reset the writer\n",
    "\n",
    "                        # Start frame extraction, cropping, and sentence generation in a separate thread\n",
    "                        executor.submit(process_video_in_background, output_file, extracted_frames_dir, processed_frames_dir, cropped_frames_dir, base_dir)\n",
    "\n",
    "        # Write frame to the video if recording\n",
    "        if recording and video_writer is not None:\n",
    "            video_writer.write(frame)\n",
    "\n",
    "        # Display the image with results\n",
    "        cv2.imshow('Hand Gesture Recognition', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "if video_writer is not None:\n",
    "    video_writer.release()\n",
    "executor.shutdown(wait=True)  # Ensure all background tasks complete\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-09T00:06:57.468056Z",
     "start_time": "2025-01-09T00:06:42.073886400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mcap\u001B[49m\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m      2\u001B[0m cv2\u001B[38;5;241m.\u001B[39mdestroyAllWindows()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'cap' is not defined"
     ]
    }
   ],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-09T00:07:22.287370100Z",
     "start_time": "2025-01-09T00:07:22.246631700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**loading dictionaries for LSTM model**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionaries loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load dictionaries from JSON files\n",
    "with open(r\"C:\\Users\\avikd\\OneDrive - Sheffield Hallam University\\Desktop\\Project & Dissertation\\8. Saved models\\text generation\\LSTM_json\\input_word_to_index.json\", \"r\") as f:\n",
    "    input_word_to_index = json.load(f)\n",
    "\n",
    "with open(r\"C:\\Users\\avikd\\OneDrive - Sheffield Hallam University\\Desktop\\Project & Dissertation\\8. Saved models\\text generation\\LSTM_json\\output_index_word.json\", \"r\") as f:\n",
    "    output_index_word = json.load(f)\n",
    "\n",
    "# Reverse output_index_word dictionary for lookup\n",
    "output_index_word = {int(k): v for k, v in output_index_word.items()}  # Ensure integer keys\n",
    "print(\"Dictionaries loaded successfully.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T23:04:24.870306400Z",
     "start_time": "2024-12-02T23:04:24.850563100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": "\u001B[1mModel: \"seq2seq_with_attention\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2seq_with_attention\"</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape     \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to     \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ encoder_input       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)         │          \u001B[38;5;34m0\u001B[0m │ -                 │\n│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_input       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m)      │          \u001B[38;5;34m0\u001B[0m │ -                 │\n│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ encoder_embedding   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m, \u001B[38;5;34m128\u001B[0m)    │      \u001B[38;5;34m1,408\u001B[0m │ encoder_input[\u001B[38;5;34m0\u001B[0m]… │\n│ (\u001B[38;5;33mEmbedding\u001B[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_embedding   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m) │    \u001B[38;5;34m414,464\u001B[0m │ decoder_input[\u001B[38;5;34m0\u001B[0m]… │\n│ (\u001B[38;5;33mEmbedding\u001B[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ encoder_lstm (\u001B[38;5;33mLSTM\u001B[0m) │ [(\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m),     │  \u001B[38;5;34m1,312,768\u001B[0m │ encoder_embeddin… │\n│                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m),      │            │                   │\n│                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)]      │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_lstm (\u001B[38;5;33mLSTM\u001B[0m) │ [(\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m,     │  \u001B[38;5;34m1,312,768\u001B[0m │ decoder_embeddin… │\n│                     │ \u001B[38;5;34m512\u001B[0m), (\u001B[38;5;45mNone\u001B[0m,      │            │ encoder_lstm[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n│                     │ \u001B[38;5;34m512\u001B[0m), (\u001B[38;5;45mNone\u001B[0m,      │            │ encoder_lstm[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n│                     │ \u001B[38;5;34m512\u001B[0m)]             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ attention_layer     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m) │        \u001B[38;5;34m512\u001B[0m │ decoder_lstm[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mAdditiveAttention\u001B[0m) │                   │            │ encoder_lstm[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_combined_c… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m,      │          \u001B[38;5;34m0\u001B[0m │ attention_layer[\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mConcatenate\u001B[0m)       │ \u001B[38;5;34m1024\u001B[0m)             │            │ decoder_lstm[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_dense       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m,      │  \u001B[38;5;34m3,318,950\u001B[0m │ decoder_combined… │\n│ (\u001B[38;5;33mDense\u001B[0m)             │ \u001B[38;5;34m3238\u001B[0m)             │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">414,464</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,312,768</span> │ encoder_embeddin… │\n│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),      │            │                   │\n│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]      │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,312,768</span> │ decoder_embeddin… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ attention_layer     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AdditiveAttention</span>) │                   │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_combined_c… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ attention_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_dense       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,318,950</span> │ decoder_combined… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3238</span>)             │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m6,360,872\u001B[0m (24.26 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,360,872</span> (24.26 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m6,360,870\u001B[0m (24.26 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,360,870</span> (24.26 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Optimizer params: \u001B[0m\u001B[38;5;34m2\u001B[0m (12.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Path to your saved model\n",
    "model_path = r'C:\\Users\\avikd\\OneDrive - Sheffield Hallam University\\Desktop\\Project & Dissertation\\8. Saved models\\text generation\\model0212_512.h5'\n",
    "\n",
    "# Load the model\n",
    "text_model = load_model(model_path)\n",
    "\n",
    "# Verify the model summary\n",
    "text_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T23:05:20.842523800Z",
     "start_time": "2024-12-02T23:05:20.508635100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def hybrid_sampling(prob_dist, top_k=50, top_p=0.9, temperature=1.0):\n",
    "    prob_dist = np.log(prob_dist + 1e-8) / temperature\n",
    "    prob_dist = np.exp(prob_dist)\n",
    "    prob_dist /= np.sum(prob_dist)\n",
    "\n",
    "    top_k_indices = np.argsort(prob_dist)[-top_k:]\n",
    "    top_k_probs = prob_dist[top_k_indices]\n",
    "\n",
    "    sorted_indices = np.argsort(top_k_probs)[::-1]\n",
    "    sorted_probs = top_k_probs[sorted_indices]\n",
    "    cumulative_probs = np.cumsum(sorted_probs)\n",
    "    cutoff = np.argmax(cumulative_probs >= top_p)\n",
    "    top_p_indices = top_k_indices[sorted_indices[:cutoff + 1]]\n",
    "    top_p_probs = prob_dist[top_p_indices]\n",
    "    top_p_probs /= np.sum(top_p_probs)\n",
    "\n",
    "    return np.random.choice(top_p_indices, p=top_p_probs)\n",
    "\n",
    "def generate_sentence_with_attention(model, input_sequence, output_index_word, max_seq_len=20, top_k=10, top_p=0.9, temperature=0.8):\n",
    "    decoder_input = np.zeros((1, 1))\n",
    "    decoder_input[0, 0] = output_index_word.get(\"<start>\", 1)\n",
    "\n",
    "    generated_tokens = []\n",
    "\n",
    "    for _ in range(max_seq_len):\n",
    "        predictions = model.predict([input_sequence, decoder_input])\n",
    "        prob_dist = predictions[0, -1, :]\n",
    "        next_token = hybrid_sampling(prob_dist, top_k=top_k, top_p=top_p, temperature=temperature)\n",
    "\n",
    "        if next_token == 0:\n",
    "            break\n",
    "        generated_tokens.append(next_token)\n",
    "        decoder_input = np.hstack([decoder_input, [[next_token]]])\n",
    "\n",
    "    generated_sentence = \" \".join(output_index_word.get(token, \"<unk>\") for token in generated_tokens)\n",
    "    return generated_sentence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T23:08:47.239638500Z",
     "start_time": "2024-12-02T23:08:47.218342700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**integration with LSTM model**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recording started...\n",
      "Recording stopped and saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\recording.mp4\n",
      "\n",
      "Processing video...\n",
      "Extracted 60 frames saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\extracted_frames_one\n",
      "\n",
      "Finished processing all frames. Images cropped -> 60\n",
      "\n",
      "Combined image saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\combined_frames.png\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 291ms/step\n",
      "\n",
      "Predicted word: cup\n",
      "[[0.02816016 0.36350307 0.05042779 0.0048933  0.05065876 0.00381401\n",
      "  0.06020677 0.12935103 0.30531642 0.00366876]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['encoder_input', 'decoder_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n",
      "D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (1, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 397ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 306ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\pro_dis_2\\venv\\lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (1, None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 34ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step\n",
      "Generated Sentence: is the cup next left in the dining table? table?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Recording started...\n",
      "Recording stopped and saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\recording.mp4\n",
      "\n",
      "Processing video...\n",
      "Copied last frame to fill the deficit for D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\recording.mp4.\n",
      "\n",
      "Finished processing all frames. Images cropped -> 60\n",
      "\n",
      "Combined image saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\combined_frames.png\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 56ms/step\n",
      "\n",
      "Predicted word: jump\n",
      "[[0.06115003 0.08975403 0.03717845 0.01490693 0.04032601 0.00084042\n",
      "  0.55347496 0.19282766 0.00836    0.0011815 ]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 53ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 51ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 36ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 34ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 36ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 33ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 33ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 36ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 34ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step\n",
      "Generated Sentence: didn’t the kids jump when the kids is the fruit one is now? one night? party? night? party? night? party?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Recording started...\n",
      "Recording stopped and saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\recording.mp4\n",
      "\n",
      "Processing video...\n",
      "Extracted 60 frames saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\extracted_frames_one\n",
      "\n",
      "Finished processing all frames. Images cropped -> 60\n",
      "\n",
      "Combined image saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\combined_frames.png\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step\n",
      "\n",
      "Predicted word: jump\n",
      "[[0.0089675  0.09581657 0.04365985 0.01507219 0.20739274 0.00364201\n",
      "  0.2733471  0.22382589 0.125664   0.00261213]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 30ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 30ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 30ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 33ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 36ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 34ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 30ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 34ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 33ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 34ms/step\n",
      "Generated Sentence: didn’t the kids jump when we the toy tree above you? the time? now? now? party? night? party? light? night?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Recording started...\n",
      "Recording stopped and saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\recording.mp4\n",
      "\n",
      "Processing video...\n",
      "Extracted 60 frames saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\extracted_frames_one\n",
      "\n",
      "Finished processing all frames. Images cropped -> 60\n",
      "\n",
      "Combined image saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\combined_frames.png\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 47ms/step\n",
      "\n",
      "Predicted word: pen\n",
      "[[0.00304911 0.16203909 0.03003331 0.00291975 0.06294206 0.00883552\n",
      "  0.01197534 0.27909708 0.43489394 0.0042149 ]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 34ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 32ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 33ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 33ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 32ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 32ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 29ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 30ms/step\n",
      "Generated Sentence: didn’t the pen cap fit it when it fell in the hard surface?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Recording started...\n",
      "Recording stopped and saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\recording.mp4\n",
      "\n",
      "Processing video...\n",
      "Extracted 60 frames saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\extracted_frames_one\n",
      "\n",
      "Finished processing all frames. Images cropped -> 60\n",
      "\n",
      "Combined image saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\combined_frames.png\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step\n",
      "\n",
      "Predicted word: bat\n",
      "[[7.0653963e-01 3.2561056e-02 5.5291341e-04 3.2522000e-02 5.5574309e-03\n",
      "  3.3300680e-03 8.0264248e-03 1.7411080e-01 3.0602694e-02 6.1970069e-03]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 32ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 34ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 33ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 34ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 28ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 36ms/step\n",
      "Generated Sentence: the bat i saw in the closet you last yours?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Recording started...\n",
      "Recording stopped and saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\recording.mp4\n",
      "\n",
      "Processing video...\n",
      "Extracted 60 frames saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\extracted_frames_one\n",
      "\n",
      "Finished processing all frames. Images cropped -> 60\n",
      "\n",
      "Combined image saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\combined_frames.png\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step\n",
      "\n",
      "Predicted word: bat\n",
      "[[7.9356259e-01 3.5830911e-02 2.0360600e-02 2.1780696e-02 1.7449604e-02\n",
      "  6.0216436e-04 1.7546857e-02 3.6028586e-02 4.8841368e-02 7.9966299e-03]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 34ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 29ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 29ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 32ms/step\n",
      "Generated Sentence: the bat be stored in the sports game?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Recording started...\n",
      "Recording stopped and saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\recording.mp4\n",
      "\n",
      "Processing video...\n",
      "Extracted 60 frames saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\extracted_frames_one\n",
      "\n",
      "Finished processing all frames. Images cropped -> 60\n",
      "\n",
      "Combined image saved at: D:\\PycharmProjects\\pro_dis_2\\collected_data\\!test\\combined_frames.png\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step\n",
      "\n",
      "Predicted word: pen\n",
      "[[0.23961104 0.02060456 0.00754364 0.03669753 0.00581149 0.00288769\n",
      "  0.00110493 0.01542403 0.6566106  0.01370453]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 29ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 30ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 32ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 34ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 35ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 36ms/step\n",
      "Generated Sentence: does this pen ink so writing in smooth than the paper?\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize MediaPipe Hands and Drawing modules\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "# Set the resolution to the maximum supported by your camera\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)  # Set width (e.g., 1280 for 720p)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)  # Set height (e.g., 720 for 720p)\n",
    "\n",
    "# Directories checking\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "os.makedirs(extracted_frames_dir, exist_ok=True)\n",
    "os.makedirs(cropped_frames_dir, exist_ok=True)\n",
    "combined_image_path = os.path.join(base_dir, \"combined_frames.png\")\n",
    "\n",
    "# Variables to handle recording\n",
    "recording = False\n",
    "video_writer = None\n",
    "executor = ThreadPoolExecutor(max_workers=1)  # Thread pool for frame extraction\n",
    "\n",
    "# Set video codec and file format\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30  # Default to 30 if unable to read fps\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "def is_open_hand(hand_landmarks):\n",
    "    \"\"\" Check if all fingers are extended (open hand) \"\"\"\n",
    "    for finger_tip, finger_pip in [\n",
    "        (mp_hands.HandLandmark.INDEX_FINGER_TIP, mp_hands.HandLandmark.INDEX_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.MIDDLE_FINGER_TIP, mp_hands.HandLandmark.MIDDLE_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.RING_FINGER_TIP, mp_hands.HandLandmark.RING_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.PINKY_TIP, mp_hands.HandLandmark.PINKY_PIP)\n",
    "    ]:\n",
    "        if hand_landmarks.landmark[finger_tip].y > hand_landmarks.landmark[finger_pip].y:\n",
    "            return False  # A finger is not extended\n",
    "    return True  # All fingers are extended\n",
    "\n",
    "def is_closed_fist(hand_landmarks):\n",
    "    \"\"\" Check if all fingers are folded (closed fist) \"\"\"\n",
    "    for finger_tip, finger_pip in [\n",
    "        (mp_hands.HandLandmark.INDEX_FINGER_TIP, mp_hands.HandLandmark.INDEX_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.MIDDLE_FINGER_TIP, mp_hands.HandLandmark.MIDDLE_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.RING_FINGER_TIP, mp_hands.HandLandmark.RING_FINGER_PIP),\n",
    "        (mp_hands.HandLandmark.PINKY_TIP, mp_hands.HandLandmark.PINKY_PIP)\n",
    "    ]:\n",
    "        if hand_landmarks.landmark[finger_tip].y < hand_landmarks.landmark[finger_pip].y:\n",
    "            return False  # A finger is extended\n",
    "    return True  # All fingers are folded\n",
    "\n",
    "def process_video_in_background(video_path, extracted_frames_dir, cropped_frames_dir, combined_frames_dir, target_frames=60):\n",
    "    print(f\"\\nProcessing video...\")\n",
    "\n",
    "    # Call the frame extraction function\n",
    "    extract_frames_with_priority_deletion(video_path, extracted_frames_dir, target_frames)\n",
    "\n",
    "    # Perform cropping on the extracted frames\n",
    "    frame_files = [f for f in os.listdir(extracted_frames_dir) if f.endswith('.png')]\n",
    "    cropped_count = 0\n",
    "\n",
    "    for frame_file in frame_files:\n",
    "        if process_frame(frame_file, extracted_frames_dir, cropped_frames_dir):\n",
    "            cropped_count += 1\n",
    "\n",
    "    print(f\"\\nFinished processing all frames. Images cropped -> {cropped_count}\")\n",
    "\n",
    "    # Combine cropped frames into a single image\n",
    "    combine_images(cropped_frames_dir, combined_frames_dir)\n",
    "\n",
    "    # Perform word prediction from the combined image\n",
    "    if os.path.exists(combined_image_path):\n",
    "        img = Image.open(combined_image_path).resize((224, 224), Image.LANCZOS)\n",
    "        img_array = np.array(img) / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "        predictions = lip_model.predict(img_array)\n",
    "        predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "        predicted_word = class_labels[str(predicted_class_index)]\n",
    "        print(f\"\\nPredicted word: {predicted_word}\")\n",
    "        print(predictions)\n",
    "\n",
    "        # Generate a sentence from the predicted word\n",
    "        input_sequence = np.array([[input_word_to_index[predicted_word]]])\n",
    "        generated_sentence = generate_sentence_with_attention(\n",
    "            text_model,\n",
    "            input_sequence,\n",
    "            output_index_word,\n",
    "            max_seq_len=20,\n",
    "            top_k=10,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8\n",
    "        )\n",
    "        print(f\"Generated Sentence: {generated_sentence}\")\n",
    "\n",
    "    print('----------------------------------------------------------------------')\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image_height, image_width, _ = frame.shape\n",
    "\n",
    "        # Convert BGR to RGB and flip for mirror effect\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.flip(image, 1)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        results = hands.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for num, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "                if is_open_hand(hand_landmarks):\n",
    "                    cv2.putText(image, \"Open Hand Detected\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    if not recording:\n",
    "                        recording = True\n",
    "                        video_name = \"recording.mp4\"\n",
    "                        output_file = os.path.join(base_dir, video_name)\n",
    "                        video_writer = cv2.VideoWriter(\n",
    "                            output_file,\n",
    "                            cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                            fps,\n",
    "                            (frame_width, frame_height)\n",
    "                        )\n",
    "                        if not video_writer.isOpened():\n",
    "                            print(\"Error: Video writer failed to open.\")\n",
    "                            recording = False\n",
    "                        else:\n",
    "                            print(\"\\nRecording started...\")\n",
    "\n",
    "                elif is_closed_fist(hand_landmarks):\n",
    "                    cv2.putText(image, \"Closed Fist Detected\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                    if recording:\n",
    "                        recording = False\n",
    "                        if video_writer is not None:\n",
    "                            video_writer.release()\n",
    "                            print(f\"Recording stopped and saved at: {output_file}\")\n",
    "                        video_writer = None\n",
    "\n",
    "                        executor.submit(process_video_in_background, output_file, extracted_frames_dir, cropped_frames_dir, base_dir)\n",
    "\n",
    "        if recording and video_writer is not None:\n",
    "            video_writer.write(frame)\n",
    "\n",
    "        cv2.imshow('Hand Gesture Recognition', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "if video_writer is not None:\n",
    "    video_writer.release()\n",
    "executor.shutdown(wait=True)\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T23:17:19.984221200Z",
     "start_time": "2024-12-02T23:12:56.464696300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-02T23:20:56.347133500Z",
     "start_time": "2024-12-02T23:20:56.328809Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
