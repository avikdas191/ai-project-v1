{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**for Cornell Movie Dialogs Corpus 20 words or less**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-29T14:18:10.893491600Z",
     "start_time": "2024-11-29T14:18:10.885075Z"
    }
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "movie_lines_file = r\"C:\\Users\\avikd\\Downloads\\Compressed\\archive_2\\movie_lines.txt\"  # Update with your actual path\n",
    "movie_conversations_file = r\"C:\\Users\\avikd\\Downloads\\Compressed\\archive_2\\movie_conversations.txt\"  # Update with your actual path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Reconstructing conversations...\n",
      "Filtering sentences...\n",
      "Filtered sentences saved to 'filtered_movie_sentences.txt'.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Word list with intended meanings\n",
    "target_words = {\n",
    "    \"bat\": \"a flying mammal or a sports bat\",\n",
    "    \"cup\": \"a vessel used for drinking\",\n",
    "    \"drop\": \"a small amount of liquid or to let something fall\",\n",
    "    \"eat\": \"to consume food\",\n",
    "    \"fish\": \"an aquatic animal\",\n",
    "    \"hot\": \"something with high temperature or spicy flavor\",\n",
    "    \"jump\": \"to leap into the air\",\n",
    "    \"milk\": \"a dairy product\",\n",
    "    \"pen\": \"a tool for writing\",\n",
    "    \"red\": \"a primary color\"\n",
    "}\n",
    "\n",
    "# Load movie_lines.txt\n",
    "def load_movie_lines(file_path):\n",
    "    movie_lines = {}\n",
    "    with open(file_path, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            parts = line.split(\" +++$+++ \")\n",
    "            if len(parts) == 5:\n",
    "                line_id = parts[0]\n",
    "                text = parts[-1].strip()\n",
    "                movie_lines[line_id] = text\n",
    "    return movie_lines\n",
    "\n",
    "# Load movie_conversations.txt\n",
    "def load_movie_conversations(file_path):\n",
    "    conversations = []\n",
    "    with open(file_path, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            parts = line.split(\" +++$+++ \")\n",
    "            if len(parts) == 4:\n",
    "                utterance_ids = eval(parts[-1])  # List of line IDs\n",
    "                conversations.append(utterance_ids)\n",
    "    return conversations\n",
    "\n",
    "# Reconstruct conversations using movie_lines.txt\n",
    "def reconstruct_conversations(movie_lines, conversations):\n",
    "    reconstructed_conversations = []\n",
    "    for convo in conversations:\n",
    "        convo_text = []\n",
    "        for line_id in convo:\n",
    "            if line_id in movie_lines:\n",
    "                convo_text.append(movie_lines[line_id])\n",
    "        if convo_text:\n",
    "            reconstructed_conversations.append(convo_text)\n",
    "    return reconstructed_conversations\n",
    "\n",
    "# Function to split multi-sentence text and retain only relevant parts\n",
    "def extract_relevant_sentence(text, target_words):\n",
    "    # Split the text into individual sentences, keeping the punctuation\n",
    "    sentences = re.findall(r'[^.!?]*[.!?]', text)  # Match sentences with their ending punctuation\n",
    "    relevant_sentences = []\n",
    "\n",
    "    # Check each sentence for target words\n",
    "    for sentence in sentences:\n",
    "        for word in target_words:\n",
    "            if re.search(rf\"\\b{word}\\b\", sentence.lower()):\n",
    "                relevant_sentences.append(sentence.strip())\n",
    "                break  # Stop checking once a match is found for this sentence\n",
    "\n",
    "    # Join the relevant sentences back together (if any)\n",
    "    return \" \".join(relevant_sentences).strip()\n",
    "\n",
    "# Update filter_conversations function\n",
    "def filter_conversations(conversations, target_words, max_words=20):\n",
    "    filtered_sentences = {word: {'statements': [], 'questions': []} for word in target_words.keys()}\n",
    "    for convo in conversations:\n",
    "        for text in convo:\n",
    "            # Extract the relevant part of the text\n",
    "            relevant_text = extract_relevant_sentence(text, target_words.keys())\n",
    "\n",
    "            # Skip if no relevant sentences are found\n",
    "            if not relevant_text:\n",
    "                continue\n",
    "\n",
    "            # Check word count\n",
    "            if len(relevant_text.split()) > max_words:\n",
    "                continue\n",
    "\n",
    "            # Categorize as statement or question\n",
    "            for word, meaning in target_words.items():\n",
    "                if re.search(rf\"\\b{word}\\b\", relevant_text.lower()):\n",
    "                    if relevant_text.endswith('?'):\n",
    "                        filtered_sentences[word]['questions'].append(relevant_text)\n",
    "                    else:\n",
    "                        filtered_sentences[word]['statements'].append(relevant_text)\n",
    "    return filtered_sentences\n",
    "\n",
    "# # File paths\n",
    "# movie_lines_file = \"movie_lines.txt\"  # Replace with your actual file path\n",
    "# movie_conversations_file = \"movie_conversations.txt\"  # Replace with your actual file path\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "movie_lines = load_movie_lines(movie_lines_file)\n",
    "movie_conversations = load_movie_conversations(movie_conversations_file)\n",
    "\n",
    "# Reconstruct conversations\n",
    "print(\"Reconstructing conversations...\")\n",
    "reconstructed_conversations = reconstruct_conversations(movie_lines, movie_conversations)\n",
    "\n",
    "# Filter sentences\n",
    "print(\"Filtering sentences...\")\n",
    "filtered_sentences = filter_conversations(reconstructed_conversations, target_words)\n",
    "\n",
    "# Save results to a file with UTF-8 encoding\n",
    "output_file = \"filtered_movie_sentences.txt\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for word, data in filtered_sentences.items():\n",
    "        f.write(f\"Word: {word}\\n\")\n",
    "        f.write(\"Statements:\\n\")\n",
    "        for statement in data['statements']:\n",
    "            f.write(f\"- {statement}\\n\")\n",
    "        f.write(\"\\nQuestions:\\n\")\n",
    "        for question in data['questions']:\n",
    "            f.write(f\"- {question}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Filtered sentences saved to '{output_file}'.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-29T14:43:54.567311600Z",
     "start_time": "2024-11-29T14:43:44.725200100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**for DailyDialog**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# File paths\n",
    "train_path = r'C:\\Users\\avikd\\Downloads\\Compressed\\archive_3\\train.csv'\n",
    "validation_path = r'C:\\Users\\avikd\\Downloads\\Compressed\\archive_3\\validation.csv'\n",
    "test_path = r'C:\\Users\\avikd\\Downloads\\Compressed\\archive_3\\test.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-29T20:24:45.961338300Z",
     "start_time": "2024-11-29T20:24:45.937968500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing datasets...\n",
      "Filtering dialogs...\n",
      "Combining results...\n",
      "Filtered dialogs saved to 'filtered_daily_dialog.txt'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define target words with intended meanings\n",
    "target_words = {\n",
    "    \"bat\": \"a flying mammal or a sports bat\",\n",
    "    \"cup\": \"a vessel used for drinking\",\n",
    "    \"drop\": \"a small amount of liquid or to let something fall\",\n",
    "    \"eat\": \"to consume food\",\n",
    "    \"fish\": \"an aquatic animal\",\n",
    "    \"hot\": \"something with high temperature or spicy flavor\",\n",
    "    \"jump\": \"to leap into the air\",\n",
    "    \"milk\": \"a dairy product\",\n",
    "    \"pen\": \"a tool for writing\",\n",
    "    \"red\": \"a primary color\"\n",
    "}\n",
    "\n",
    "# Function to clean and format sentences/questions\n",
    "def clean_sentence(sentence):\n",
    "    # Remove unwanted leading and trailing characters\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(r\"^[\\[\\]'\\\" ]+\", \"\", sentence)  # Remove leading brackets, quotes, and spaces\n",
    "    sentence = re.sub(r\"[\\[\\]'\\\" ]+$\", \"\", sentence)  # Remove trailing brackets, quotes, and spaces\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)  # Standardize whitespace\n",
    "\n",
    "    # Return None for invalid or empty sentences\n",
    "    if not sentence or sentence in [\"'\", '\"', \"-\", \"[\", \"]\"]:\n",
    "        return None\n",
    "\n",
    "    return sentence\n",
    "\n",
    "# Function to filter sentences/questions based on target words and length\n",
    "def filter_sentences(dialogs, target_words, max_words=20):\n",
    "    filtered_sentences = {word: {'statements': [], 'questions': []} for word in target_words.keys()}\n",
    "\n",
    "    for dialog in dialogs:\n",
    "        # Parse dialog string into individual sentences\n",
    "        sentences = re.findall(r'[^.!?]*[.!?]', dialog)  # Retain punctuation\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Clean the sentence\n",
    "            cleaned_sentence = clean_sentence(sentence)\n",
    "            if not cleaned_sentence:\n",
    "                continue  # Skip invalid or empty sentences\n",
    "\n",
    "            # Check for word count limit\n",
    "            if len(cleaned_sentence.split()) > max_words:\n",
    "                continue  # Skip sentences longer than the word limit\n",
    "\n",
    "            # Check for target words\n",
    "            for word in target_words.keys():\n",
    "                if re.search(rf\"\\b{word}\\b\", cleaned_sentence.lower()):\n",
    "                    # Categorize as statement or question\n",
    "                    if cleaned_sentence.endswith('?'):\n",
    "                        filtered_sentences[word]['questions'].append(cleaned_sentence)\n",
    "                    else:\n",
    "                        filtered_sentences[word]['statements'].append(cleaned_sentence)\n",
    "\n",
    "    return filtered_sentences\n",
    "\n",
    "# Load and process each file\n",
    "def process_csv(file_path):\n",
    "    # Load the CSV file\n",
    "    data = pd.read_csv(file_path, header=None, encoding='utf-8')\n",
    "\n",
    "    # Extract the dialog column\n",
    "    dialogs = data.iloc[:, 0]  # Assuming dialogs are in the first column\n",
    "    return dialogs\n",
    "\n",
    "# Combine results from all datasets\n",
    "def combine_results(*datasets):\n",
    "    combined_results = {word: {'statements': [], 'questions': []} for word in target_words.keys()}\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for word in target_words.keys():\n",
    "            combined_results[word]['statements'].extend(dataset[word]['statements'])\n",
    "            combined_results[word]['questions'].extend(dataset[word]['questions'])\n",
    "\n",
    "    return combined_results\n",
    "\n",
    "# File paths\n",
    "train_path = r'C:\\Users\\avikd\\Downloads\\Compressed\\archive_3\\train.csv'\n",
    "validation_path = r'C:\\Users\\avikd\\Downloads\\Compressed\\archive_3\\validation.csv'\n",
    "test_path = r'C:\\Users\\avikd\\Downloads\\Compressed\\archive_3\\test.csv'\n",
    "\n",
    "# Process datasets\n",
    "print(\"Processing datasets...\")\n",
    "train_dialogs = process_csv(train_path)\n",
    "validation_dialogs = process_csv(validation_path)\n",
    "test_dialogs = process_csv(test_path)\n",
    "\n",
    "# Apply filtering\n",
    "print(\"Filtering dialogs...\")\n",
    "filtered_train = filter_sentences(train_dialogs, target_words, max_words=20)\n",
    "filtered_validation = filter_sentences(validation_dialogs, target_words, max_words=20)\n",
    "filtered_test = filter_sentences(test_dialogs, target_words, max_words=20)\n",
    "\n",
    "# Combine results\n",
    "print(\"Combining results...\")\n",
    "combined_results = combine_results(filtered_train, filtered_validation, filtered_test)\n",
    "\n",
    "# Save the filtered results to a file\n",
    "output_file = \"filtered_daily_dialog.txt\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for word, data in combined_results.items():\n",
    "        f.write(f\"Word: {word}\\n\")\n",
    "        f.write(\"Statements:\\n\")\n",
    "        for statement in data['statements']:\n",
    "            cleaned_statement = clean_sentence(statement)\n",
    "            if cleaned_statement:  # Ensure the statement is valid after cleaning\n",
    "                f.write(f\"- {cleaned_statement}\\n\")  # Single dash followed by one space\n",
    "        f.write(\"\\nQuestions:\\n\")\n",
    "        for question in data['questions']:\n",
    "            cleaned_question = clean_sentence(question)\n",
    "            if cleaned_question:  # Ensure the question is valid after cleaning\n",
    "                f.write(f\"- {cleaned_question}\\n\")  # Single dash followed by one space\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Filtered dialogs saved to '{output_file}'.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-29T20:57:33.979316100Z",
     "start_time": "2024-11-29T20:57:30.994450Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**for QuAC (Question Answering in Context) 20 words or less**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# File paths for train and validation JSON files\n",
    "train_file_path = r'C:\\Users\\avikd\\Downloads\\Compressed\\archive_4\\train_v2.json'\n",
    "validation_file_path = r'C:\\Users\\avikd\\Downloads\\Compressed\\archive_4\\val_v2.json'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-29T15:23:51.018974400Z",
     "start_time": "2024-11-29T15:23:50.998304600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining results from train and validation...\n",
      "Processing training data...\n",
      "Processing validation data...\n",
      "Filtered results saved to 'filtered_quac_combined_20words_or_less.txt'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Define the target words with their intended meanings\n",
    "target_words = {\n",
    "    \"bat\": \"a flying mammal or a sports bat\",\n",
    "    \"cup\": \"a vessel used for drinking\",\n",
    "    \"drop\": \"a small amount of liquid or to let something fall\",\n",
    "    \"eat\": \"to consume food\",\n",
    "    \"fish\": \"an aquatic animal\",\n",
    "    \"hot\": \"something with high temperature or spicy flavor\",\n",
    "    \"jump\": \"to leap into the air\",\n",
    "    \"milk\": \"a dairy product\",\n",
    "    \"pen\": \"a tool for writing\",\n",
    "    \"red\": \"a primary color\"\n",
    "}\n",
    "\n",
    "# Function to extract relevant sentences containing target words\n",
    "def extract_relevant_sentences(text, target_words, max_words=20):\n",
    "    sentences = re.findall(r'[^.!?]*[.!?]', text)  # Split text into individual sentences\n",
    "    relevant_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence.split()) > max_words:\n",
    "            continue  # Skip sentences with more than max_words\n",
    "        for word in target_words.keys():\n",
    "            if re.search(rf\"\\b{word}\\b\", sentence.lower()):\n",
    "                relevant_sentences.append(sentence.strip())\n",
    "                break  # Include the sentence only once per match\n",
    "    return relevant_sentences\n",
    "\n",
    "# Function to process a QuAC JSON file and extract relevant sentences/questions\n",
    "def process_quac_file(file_path, target_words, max_words=20):\n",
    "    filtered_sentences = {word: {'statements': [], 'questions': []} for word in target_words.keys()}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for entry in data['data']:\n",
    "        for paragraph in entry['paragraphs']:\n",
    "            # Extract sentences from context\n",
    "            context_sentences = extract_relevant_sentences(paragraph['context'], target_words, max_words)\n",
    "            for sentence in context_sentences:\n",
    "                for word in target_words.keys():\n",
    "                    if re.search(rf\"\\b{word}\\b\", sentence.lower()):\n",
    "                        if sentence.endswith('?'):\n",
    "                            filtered_sentences[word]['questions'].append(sentence)\n",
    "                        else:\n",
    "                            filtered_sentences[word]['statements'].append(sentence)\n",
    "\n",
    "            # Extract sentences from questions\n",
    "            for qa in paragraph['qas']:\n",
    "                question_sentences = extract_relevant_sentences(qa['question'], target_words, max_words)\n",
    "                for sentence in question_sentences:\n",
    "                    for word in target_words.keys():\n",
    "                        if re.search(rf\"\\b{word}\\b\", sentence.lower()):\n",
    "                            filtered_sentences[word]['questions'].append(sentence)\n",
    "\n",
    "    return filtered_sentences\n",
    "\n",
    "# Combine results from train and validation files into one\n",
    "def combine_results(train_file, validation_file, target_words, max_words=20):\n",
    "    print(\"Processing training data...\")\n",
    "    train_results = process_quac_file(train_file, target_words, max_words)\n",
    "\n",
    "    print(\"Processing validation data...\")\n",
    "    validation_results = process_quac_file(validation_file, target_words, max_words)\n",
    "\n",
    "    combined_results = {word: {'statements': [], 'questions': []} for word in target_words.keys()}\n",
    "    for word in target_words.keys():\n",
    "        combined_results[word]['statements'] = train_results[word]['statements'] + validation_results[word]['statements']\n",
    "        combined_results[word]['questions'] = train_results[word]['questions'] + validation_results[word]['questions']\n",
    "\n",
    "    return combined_results\n",
    "\n",
    "# File paths for train and validation JSON files\n",
    "# train_file_path = '/path/to/train_v0.2.json'\n",
    "# validation_file_path = '/path/to/val_v0.2.json'\n",
    "\n",
    "# Process and combine the results\n",
    "print(\"Combining results from train and validation...\")\n",
    "combined_results = combine_results(train_file_path, validation_file_path, target_words)\n",
    "\n",
    "# Save combined results to a single text file\n",
    "output_file = \"filtered_quac_combined_20words_or_less.txt\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for word, data in combined_results.items():\n",
    "        f.write(f\"Word: {word}\\n\")\n",
    "        f.write(\"Statements:\\n\")\n",
    "        for statement in data['statements']:\n",
    "            f.write(f\"- {statement}\\n\")\n",
    "        f.write(\"\\nQuestions:\\n\")\n",
    "        for question in data['questions']:\n",
    "            f.write(f\"- {question}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Filtered results saved to '{output_file}'.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-29T15:31:46.003260300Z",
     "start_time": "2024-11-29T15:31:40.639472500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**for Persona-Chat 20 words or less**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the Persona-Chat dataset...\n",
      "Processing training data...\n",
      "Processing validation data...\n",
      "Filtered results saved to 'filtered_persona_chat.txt'.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# Define the target words with their intended meanings\n",
    "target_words = {\n",
    "    \"bat\": \"a flying mammal or a sports bat\",\n",
    "    \"cup\": \"a vessel used for drinking\",\n",
    "    \"drop\": \"a small amount of liquid or to let something fall\",\n",
    "    \"eat\": \"to consume food\",\n",
    "    \"fish\": \"an aquatic animal\",\n",
    "    \"hot\": \"something with high temperature or spicy flavor\",\n",
    "    \"jump\": \"to leap into the air\",\n",
    "    \"milk\": \"a dairy product\",\n",
    "    \"pen\": \"a tool for writing\",\n",
    "    \"red\": \"a primary color\"\n",
    "}\n",
    "\n",
    "# Function to extract relevant sentences containing target words\n",
    "def extract_relevant_sentences(text, target_words, max_words=20):\n",
    "    if not isinstance(text, str):\n",
    "        return []  # Skip non-string entries\n",
    "    sentences = re.findall(r'[^.!?]*[.!?]', text)  # Split text into individual sentences\n",
    "    relevant_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence.split()) > max_words:\n",
    "            continue  # Skip sentences with more than max_words\n",
    "        for word in target_words.keys():\n",
    "            if re.search(rf\"\\b{word}\\b\", sentence.lower()):\n",
    "                relevant_sentences.append(sentence.strip())\n",
    "                break  # Include the sentence only once per match\n",
    "    return relevant_sentences\n",
    "\n",
    "# Process a Persona-Chat dataset split and extract relevant data\n",
    "def process_persona_chat(dataset_split, target_words, max_words=20):\n",
    "    filtered_sentences = {word: {'statements': [], 'questions': []} for word in target_words.keys()}\n",
    "    for dialog in dataset_split:\n",
    "        # Process persona descriptions and utterances\n",
    "        all_text = []\n",
    "        if 'personality' in dialog and isinstance(dialog['personality'], list):\n",
    "            all_text.extend(dialog['personality'])\n",
    "        if 'utterances' in dialog and isinstance(dialog['utterances'], list):\n",
    "            all_text.extend([utt['text'] for utt in dialog['utterances'] if 'text' in utt])\n",
    "\n",
    "        for text in all_text:\n",
    "            # Extract relevant sentences\n",
    "            relevant_sentences = extract_relevant_sentences(text, target_words, max_words)\n",
    "            for sentence in relevant_sentences:\n",
    "                for word in target_words.keys():\n",
    "                    if re.search(rf\"\\b{word}\\b\", sentence.lower()):\n",
    "                        if sentence.endswith('?'):\n",
    "                            filtered_sentences[word]['questions'].append(sentence)\n",
    "                        else:\n",
    "                            filtered_sentences[word]['statements'].append(sentence)\n",
    "    return filtered_sentences\n",
    "\n",
    "# Load the Persona-Chat dataset from Hugging Face\n",
    "print(\"Downloading the Persona-Chat dataset...\")\n",
    "dataset = load_dataset('AlekseyKorshuk/persona-chat')\n",
    "\n",
    "# Process both train and validation splits\n",
    "print(\"Processing training data...\")\n",
    "train_results = process_persona_chat(dataset['train'], target_words)\n",
    "\n",
    "print(\"Processing validation data...\")\n",
    "validation_results = process_persona_chat(dataset['validation'], target_words)\n",
    "\n",
    "# Combine results from train and validation\n",
    "combined_results = {word: {'statements': [], 'questions': []} for word in target_words.keys()}\n",
    "for word in target_words.keys():\n",
    "    combined_results[word]['statements'] = train_results[word]['statements'] + validation_results[word]['statements']\n",
    "    combined_results[word]['questions'] = train_results[word]['questions'] + validation_results[word]['questions']\n",
    "\n",
    "# Save results to a single text file\n",
    "output_file = \"filtered_persona_chat.txt\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for word, data in combined_results.items():\n",
    "        f.write(f\"Word: {word}\\n\")\n",
    "        f.write(\"Statements:\\n\")\n",
    "        for statement in data['statements']:\n",
    "            f.write(f\"- {statement}\\n\")\n",
    "        f.write(\"\\nQuestions:\\n\")\n",
    "        for question in data['questions']:\n",
    "            f.write(f\"- {question}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Filtered results saved to '{output_file}'.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-29T15:43:58.825124500Z",
     "start_time": "2024-11-29T15:43:50.588542500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved to D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\2_csv_data\\output.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = r\"D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\2_csv_data\\4_Persona_Chat.csv\"  # Replace with your input file path\n",
    "output_file = r\"D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\2_csv_data\\output.csv\"  # Replace with your desired output file path\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# Ensure the CSV has at least two columns\n",
    "if len(data.columns) < 2:\n",
    "    print(\"The CSV file does not have enough columns.\")\n",
    "else:\n",
    "    # Extract the column names\n",
    "    word_column = data.columns[0]\n",
    "    sentence_column = data.columns[1]\n",
    "\n",
    "    # Capitalize the first letter of each sentence/question in the second column\n",
    "    data[sentence_column] = data[sentence_column].apply(lambda x: x[0].upper() + x[1:] if isinstance(x, str) else x)\n",
    "\n",
    "    # Save the updated data to a new CSV file\n",
    "    data.to_csv(output_file, index=False)\n",
    "    print(f\"Processed file saved to {output_file}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T14:18:21.419061300Z",
     "start_time": "2024-12-01T14:18:18.925504600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**for OpenSubtitle 20 words or less**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# File path to the en_txt file\n",
    "file_path = r'C:\\Users\\avikd\\Downloads\\Compressed\\en_txt\\en.txt'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-29T16:32:08.562593200Z",
     "start_time": "2024-11-29T16:32:08.553076200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the subtitles file...\n",
      "Filtered results saved to 'filtered_open_subtitles.txt'.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the target words with their intended meanings\n",
    "target_words = {\n",
    "    \"bat\": \"a flying mammal or a sports bat\",\n",
    "    \"cup\": \"a vessel used for drinking\",\n",
    "    \"drop\": \"a small amount of liquid or to let something fall\",\n",
    "    \"eat\": \"to consume food\",\n",
    "    \"fish\": \"an aquatic animal\",\n",
    "    \"hot\": \"something with high temperature or spicy flavor\",\n",
    "    \"jump\": \"to leap into the air\",\n",
    "    \"milk\": \"a dairy product\",\n",
    "    \"pen\": \"a tool for writing\",\n",
    "    \"red\": \"a primary color\"\n",
    "}\n",
    "\n",
    "# Function to extract relevant sentences containing target words\n",
    "def extract_relevant_sentences(file_path, target_words, max_words=20):\n",
    "    filtered_sentences = {word: {'statements': [], 'questions': []} for word in target_words.keys()}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Strip whitespace and skip empty lines\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Split text into sentences\n",
    "            sentences = re.findall(r'[^.!?]*[.!?]', line)  # Match sentences with their punctuation\n",
    "            for sentence in sentences:\n",
    "                # Skip sentences longer than max_words\n",
    "                if len(sentence.split()) > max_words:\n",
    "                    continue\n",
    "\n",
    "                # Check for target words\n",
    "                for word in target_words.keys():\n",
    "                    if re.search(rf\"\\b{word}\\b\", sentence.lower()):\n",
    "                        if sentence.endswith('?'):\n",
    "                            filtered_sentences[word]['questions'].append(sentence.strip())\n",
    "                        else:\n",
    "                            filtered_sentences[word]['statements'].append(sentence.strip())\n",
    "                        break  # Avoid duplicating the sentence for multiple word matches\n",
    "    return filtered_sentences\n",
    "\n",
    "# # File path to the en_txt file\n",
    "# file_path = '/path/to/en_txt'\n",
    "\n",
    "# Extract relevant sentences\n",
    "print(\"Processing the subtitles file...\")\n",
    "filtered_data = extract_relevant_sentences(file_path, target_words)\n",
    "\n",
    "# Save the filtered sentences to a file\n",
    "output_file = \"filtered_open_subtitles.txt\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for word, data in filtered_data.items():\n",
    "        f.write(f\"Word: {word}\\n\")\n",
    "        f.write(\"Statements:\\n\")\n",
    "        for statement in data['statements']:\n",
    "            f.write(f\"- {statement}\\n\")\n",
    "        f.write(\"\\nQuestions:\\n\")\n",
    "        for question in data['questions']:\n",
    "            f.write(f\"- {question}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Filtered results saved to '{output_file}'.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-29T18:00:08.819087800Z",
     "start_time": "2024-11-29T16:32:14.400347Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**converting extracted text files into csv**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# Folder paths\n",
    "input_folder = r\"D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\1_raw_data\"  # Replace with the folder containing your .txt files\n",
    "output_folder = r\"D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\2_csv_data\"  # Replace with your desired output folder for .csv files"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-29T21:00:34.626797800Z",
     "start_time": "2024-11-29T21:00:34.596795600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1_Cornell Movie Dialogs Corpus.txt...\n",
      "Saved CSV as D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\2_csv_data\\1_Cornell Movie Dialogs Corpus.csv\n",
      "\n",
      "Processing 2_DailyDialog.txt...\n",
      "Saved CSV as D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\2_csv_data\\2_DailyDialog.csv\n",
      "\n",
      "Processing 3_QuAC (Question Answering in Context).txt...\n",
      "Saved CSV as D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\2_csv_data\\3_QuAC (Question Answering in Context).csv\n",
      "\n",
      "Processing 4_Persona_Chat.txt...\n",
      "Saved CSV as D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\2_csv_data\\4_Persona_Chat.csv\n",
      "\n",
      "Processing 5_OpenSubtitles.txt...\n",
      "Saved CSV as D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\2_csv_data\\5_OpenSubtitles.csv\n",
      "\n",
      "\n",
      "Processing completed! Check 'D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\2_csv_data\\skipped_lines.log' for skipped lines.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Function to parse a text file and prepare rows for CSV\n",
    "def convert_text_to_csv(input_file):\n",
    "    rows = []  # List to store rows for the CSV\n",
    "    skipped_lines = []  # List to store skipped lines for logging\n",
    "    current_word = None\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line_number, line in enumerate(f, 1):  # Keep track of line numbers\n",
    "            line = line.strip()\n",
    "\n",
    "            if line.startswith(\"Word:\"):\n",
    "                # Extract the current word\n",
    "                current_word = line.replace(\"Word:\", \"\").strip()\n",
    "                if not current_word:\n",
    "                    skipped_lines.append((line_number, line))  # Log empty word\n",
    "            elif line.startswith(\"-\") and current_word:\n",
    "                # Extract the sentence/question\n",
    "                sentence = line[1:].strip()\n",
    "                if sentence:\n",
    "                    rows.append([current_word, sentence])  # Add to rows\n",
    "                else:\n",
    "                    skipped_lines.append((line_number, line))  # Log empty sentence\n",
    "            elif line:  # Non-empty line that doesn't match expected formats\n",
    "                skipped_lines.append((line_number, line))  # Log unexpected line\n",
    "\n",
    "    return rows, skipped_lines\n",
    "\n",
    "# Function to process all text files in a folder and save as CSV\n",
    "def process_folder_to_csv(input_folder, output_folder, log_file=\"skipped_lines.log\"):\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Prepare log file\n",
    "    log_path = os.path.join(output_folder, log_file)\n",
    "    with open(log_path, 'w', encoding='utf-8') as log:\n",
    "        log.write(\"Skipped Lines Log:\\n\")\n",
    "        log.write(\"=================\\n\")\n",
    "\n",
    "    # Iterate through all .txt files in the folder\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            input_file = os.path.join(input_folder, file_name)\n",
    "            output_file = os.path.join(output_folder, file_name.replace(\".txt\", \".csv\"))\n",
    "\n",
    "            # Convert the text file to CSV rows\n",
    "            print(f\"Processing {file_name}...\")\n",
    "            csv_rows, skipped_lines = convert_text_to_csv(input_file)\n",
    "\n",
    "            # Save the rows to a CSV file\n",
    "            with open(output_file, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "                writer = csv.writer(csv_file)\n",
    "                # Write header\n",
    "                writer.writerow([\"Word\", \"Sentence/Question\"])\n",
    "                # Write data rows\n",
    "                writer.writerows(csv_rows)\n",
    "\n",
    "            print(f\"Saved CSV as {output_file}\\n\")\n",
    "\n",
    "            # Log skipped lines\n",
    "            with open(log_path, 'a', encoding='utf-8') as log:\n",
    "                if skipped_lines:\n",
    "                    log.write(f\"\\nFile: {file_name}\\n\")\n",
    "                    for line_number, line in skipped_lines:\n",
    "                        log.write(f\"Line {line_number}: {line}\\n\")\n",
    "                else:\n",
    "                    log.write(f\"\\nFile: {file_name} - No skipped lines\\n\")\n",
    "\n",
    "    print(f\"\\nProcessing completed! Check '{log_path}' for skipped lines.\")\n",
    "\n",
    "# Process all .txt files in the input folder\n",
    "process_folder_to_csv(input_folder, output_folder)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-29T21:01:01.485224300Z",
     "start_time": "2024-11-29T21:00:56.072033Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# Input and output folder paths\n",
    "input_folder = r\"D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\2_csv_data\"  # Folder containing the input CSV files\n",
    "output_folder = r\"D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\3_processed_csv_data\"  # Folder to save the processed CSV files\n",
    "log_file_name = \"deleted_rows_log.txt\"  # Log file for deleted rows"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-29T21:41:39.671859300Z",
     "start_time": "2024-11-29T21:41:39.650298700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\2_csv_data\\1_Cornell Movie Dialogs Corpus.csv. Saved cleaned data to D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\3_processed_csv_data\\1_Cornell Movie Dialogs Corpus.csv.\n",
      "Rows removed: 124, Duplicates removed: 92\n",
      "\n",
      "Processed D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\2_csv_data\\2_DailyDialog.csv. Saved cleaned data to D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\3_processed_csv_data\\2_DailyDialog.csv.\n",
      "Rows removed: 5, Duplicates removed: 260\n",
      "\n",
      "Processed D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\2_csv_data\\3_QuAC (Question Answering in Context).csv. Saved cleaned data to D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\3_processed_csv_data\\3_QuAC (Question Answering in Context).csv.\n",
      "Rows removed: 0, Duplicates removed: 559\n",
      "\n",
      "Processed D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\2_csv_data\\4_Persona_Chat.csv. Saved cleaned data to D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\3_processed_csv_data\\4_Persona_Chat.csv.\n",
      "Rows removed: 0, Duplicates removed: 2277\n",
      "\n",
      "Processed D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\2_csv_data\\5_OpenSubtitles.csv. Saved cleaned data to D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\3_processed_csv_data\\5_OpenSubtitles.csv.\n",
      "Rows removed: 205279, Duplicates removed: 1339468\n",
      "\n",
      "Deleted rows log saved to D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\3_processed_csv_data\\deleted_rows_log.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Path for the deleted rows log file\n",
    "log_file_path = os.path.join(output_folder, log_file_name)\n",
    "\n",
    "# Initialize a list to store deleted rows for logging\n",
    "deleted_rows_log = []\n",
    "\n",
    "# Function to clean the Sentence/Question column\n",
    "def clean_sentence(sentence):\n",
    "    # Remove unwanted leading characters\n",
    "    sentence = re.sub(r\"^[-\\[\\]: ]+\", \"\", sentence.strip())  # Remove unwanted starts\n",
    "    return sentence.strip()\n",
    "\n",
    "# Function to check for unrecognized content\n",
    "def is_unrecognized_content(value):\n",
    "    return value.strip().lower() in [\"#name?\", \"#value!\", \"nan\", \"\"]\n",
    "\n",
    "# Function to process CSV files\n",
    "def process_csv(file_path, output_path):\n",
    "    # Load the CSV file\n",
    "    data = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "\n",
    "    # Ensure the 2nd column (Sentence/Question) exists\n",
    "    if len(data.columns) < 2:\n",
    "        print(f\"File {file_path} does not have enough columns. Skipping...\")\n",
    "        return\n",
    "\n",
    "    # Extract the Sentence/Question column (2nd column)\n",
    "    column_name = data.columns[1]  # Assume the 2nd column contains sentences/questions\n",
    "\n",
    "    # Initialize a DataFrame for rows to delete\n",
    "    rows_to_delete = []\n",
    "\n",
    "    # Clean the Sentence/Question column\n",
    "    for index, row in data.iterrows():\n",
    "        sentence = str(row[column_name])\n",
    "\n",
    "        # Check for unrecognized content\n",
    "        if is_unrecognized_content(sentence):\n",
    "            rows_to_delete.append(row)\n",
    "            continue\n",
    "\n",
    "        # Clean the sentence\n",
    "        cleaned_sentence = clean_sentence(sentence)\n",
    "\n",
    "        # Skip rows with invalid starts or empty content after cleaning\n",
    "        if not cleaned_sentence or len(cleaned_sentence.split()) < 3:\n",
    "            rows_to_delete.append(row)\n",
    "            continue\n",
    "\n",
    "        # Update the sentence in the DataFrame\n",
    "        data.at[index, column_name] = cleaned_sentence\n",
    "\n",
    "    # Remove rows to delete\n",
    "    rows_to_delete_df = pd.DataFrame(rows_to_delete, columns=data.columns)\n",
    "    data = data.drop(rows_to_delete_df.index)\n",
    "\n",
    "    # Remove duplicates\n",
    "    before_deduplication = len(data)\n",
    "    data = data.drop_duplicates(subset=[column_name], keep=\"first\")\n",
    "    duplicates_removed = before_deduplication - len(data)\n",
    "\n",
    "    # Log deleted rows\n",
    "    deleted_rows_log.extend(rows_to_delete_df.values.tolist())\n",
    "\n",
    "    # Save the cleaned CSV\n",
    "    data.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Processed {file_path}. Saved cleaned data to {output_path}.\")\n",
    "    print(f\"Rows removed: {len(rows_to_delete)}, Duplicates removed: {duplicates_removed}\\n\")\n",
    "\n",
    "# Process all CSV files in the folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        input_path = os.path.join(input_folder, file_name)\n",
    "        output_path = os.path.join(output_folder, file_name)\n",
    "        process_csv(input_path, output_path)\n",
    "\n",
    "# Save the log of deleted rows\n",
    "with open(log_file_path, \"w\", encoding=\"utf-8\") as log_file:\n",
    "    log_file.write(\"Deleted Rows Log\\n\")\n",
    "    log_file.write(\"================\\n\")\n",
    "    for row in deleted_rows_log:\n",
    "        log_file.write(f\"{row}\\n\")\n",
    "\n",
    "print(f\"Deleted rows log saved to {log_file_path}.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-29T22:07:22.338731400Z",
     "start_time": "2024-11-29T22:05:20.603288100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Cleaning extracted sentences/question from the datasets**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\2_csv_data\\1_Cornell Movie Dialogs Corpus.csv. Saved cleaned data to D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\3_processed_csv\\1_Cornell Movie Dialogs Corpus.csv. Rows removed: 1212.\n",
      "\n",
      "Processed D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\2_csv_data\\2_DailyDialog.csv. Saved cleaned data to D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\3_processed_csv\\2_DailyDialog.csv. Rows removed: 718.\n",
      "\n",
      "Processed D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\2_csv_data\\3_QuAC (Question Answering in Context).csv. Saved cleaned data to D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\3_processed_csv\\3_QuAC (Question Answering in Context).csv. Rows removed: 1310.\n",
      "\n",
      "Processed D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\2_csv_data\\4_Persona_Chat.csv. Saved cleaned data to D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\3_processed_csv\\4_Persona_Chat.csv. Rows removed: 2301.\n",
      "\n",
      "Processed D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\2_csv_data\\5_OpenSubtitles.csv. Saved cleaned data to D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\3_processed_csv\\5_OpenSubtitles.csv. Rows removed: 1847259.\n",
      "\n",
      "Processing completed. All cleaned files are saved in the output folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Input and output folder paths\n",
    "input_folder = r\"D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\2_csv_data\"  # Folder containing input CSV files\n",
    "output_folder = r\"D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\3_processed_csv\"  # Folder to save the processed CSV files\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to clean subtitle text\n",
    "def clean_subtitle_text(text):\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.replace('â€™', \"'\").replace('â€œ', '\"').replace('â€�', '\"').replace('â€“', '-').replace('Ã©', 'e')\n",
    "    contractions = {\"don t\": \"don't\", \"can t\": \"can't\", \"i m\": \"i'm\", \"it s\": \"it's\", \"you re\": \"you're\",\n",
    "                    \"we re\": \"we're\", \"they re\": \"they're\"}\n",
    "    for k, v in contractions.items():\n",
    "        text = re.sub(rf'\\b{k}\\b', v, text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,!?'\\\"]+\", \" \", text)\n",
    "    text = re.sub(r\" (\\.|\\?|!)\", r\"\\1\", text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Function to check for unwanted patterns\n",
    "def has_unwanted_patterns(text):\n",
    "    if \"#NAME?\" in text:\n",
    "        return True\n",
    "    if re.search(r\"\\d+(-\\d+)+-?\", text):\n",
    "        return True\n",
    "    if re.search(r'(\\'{2,}|\"{\"2,})', text):\n",
    "        return True\n",
    "    if re.search(r\"<[^>]+>\", text):\n",
    "        return True\n",
    "    if re.search(r\"\\{[^}]*\\}|\\[[^\\]]*\\]|\\([^)]*\\)\", text):\n",
    "        return True\n",
    "    if \"Nº\" in text:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to check for invalid start conditions\n",
    "def starts_with_invalid_character(text):\n",
    "    if re.match(r\"^[-_]\", text):\n",
    "        return True\n",
    "    if re.match(r\"^\\d\", text):\n",
    "        return True\n",
    "    if re.match(r\"^[a-z]\", text):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to check for invalid start or end conditions\n",
    "def has_invalid_start_or_end(text):\n",
    "    if re.match(r\"^[^a-zA-Z0-9.!?]\", text) or re.search(r\"[^a-zA-Z0-9.!?]$\", text):\n",
    "        return True\n",
    "    if re.match(r\"^[\\\"']\", text) or re.search(r\"[\\\"']$\", text):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to check if the word in column 1 is repeated in column 2\n",
    "def is_word_repeated(word, sentence):\n",
    "    word_count = len(re.findall(rf\"\\b{word.lower()}\\b\", sentence.lower()))\n",
    "    return word_count > 1\n",
    "\n",
    "# Function to remove unwanted special characters\n",
    "def remove_unwanted_characters(text):\n",
    "    text = re.sub(r\"\\b#\\d+\\b\", \"\", text)\n",
    "    text = text.replace(\"Â\", \"\").replace(\"*\", \"\")\n",
    "    return text.strip()\n",
    "\n",
    "# Function to check for multiple sentences/questions in a cell\n",
    "def contains_multiple_sentences(text):\n",
    "    return len(re.findall(r\"[.!?]\", text)) > 1\n",
    "\n",
    "# Function to check for all-uppercase sentences/questions\n",
    "def is_all_uppercase(sentence):\n",
    "    words = sentence.split()\n",
    "    return all(word.isupper() for word in words)\n",
    "\n",
    "# Function to count words more accurately\n",
    "def count_words(sentence):\n",
    "    # Remove leading/trailing spaces and split based on whitespace\n",
    "    words = sentence.strip().split()\n",
    "    # Consider words as valid if they contain alphanumeric characters\n",
    "    valid_words = [word for word in words if re.search(r\"[a-zA-Z0-9]\", word)]\n",
    "    return len(valid_words)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path, output_path):\n",
    "    data = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "    if len(data.columns) < 2:\n",
    "        print(f\"File {file_path} does not have enough columns. Skipping...\")\n",
    "        return\n",
    "\n",
    "    word_column = data.columns[0]\n",
    "    sentence_column = data.columns[1]\n",
    "    rows_to_delete = []\n",
    "    unique_sentences = set()\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        word = str(row[word_column])\n",
    "        sentence = str(row[sentence_column])\n",
    "        if has_unwanted_patterns(sentence):\n",
    "            rows_to_delete.append(index)\n",
    "            continue\n",
    "        if starts_with_invalid_character(sentence):\n",
    "            rows_to_delete.append(index)\n",
    "            continue\n",
    "        if has_invalid_start_or_end(sentence):\n",
    "            rows_to_delete.append(index)\n",
    "            continue\n",
    "        if contains_multiple_sentences(sentence):\n",
    "            rows_to_delete.append(index)\n",
    "            continue\n",
    "        if is_word_repeated(word, sentence):\n",
    "            rows_to_delete.append(index)\n",
    "            continue\n",
    "        if count_words(sentence) > 10 or count_words(sentence) < 4:   # Updated logic in the main loop\n",
    "            rows_to_delete.append(index)\n",
    "            continue\n",
    "        if is_all_uppercase(sentence):  # Check for all-uppercase sentences\n",
    "            rows_to_delete.append(index)\n",
    "            continue\n",
    "        if sentence in unique_sentences:\n",
    "            rows_to_delete.append(index)\n",
    "            continue\n",
    "        unique_sentences.add(sentence)\n",
    "        cleaned_sentence = clean_subtitle_text(remove_unwanted_characters(sentence))\n",
    "        data.at[index, sentence_column] = cleaned_sentence\n",
    "\n",
    "    data = data.drop(rows_to_delete)\n",
    "    data.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Processed {file_path}. Saved cleaned data to {output_path}. Rows removed: {len(rows_to_delete)}.\\n\")\n",
    "\n",
    "# Process all CSV files in the folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        input_path = os.path.join(input_folder, file_name)\n",
    "        output_path = os.path.join(output_folder, file_name)\n",
    "        process_csv(input_path, output_path)\n",
    "\n",
    "print(\"Processing completed. All cleaned files are saved in the output folder.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T20:31:20.000381300Z",
     "start_time": "2024-12-01T20:28:47.892311700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**from processed csv file extract sentence with listed words and saved as .txt format**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed. Word-specific files are saved in D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\4_word_txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output folder paths\n",
    "processed_folder = r\"D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\3_processed_csv\"  # Folder containing processed CSV files\n",
    "output_folder = r\"D:\\PycharmProjects\\pro_dis_2\\data_4_txt_gen\\data_2nd\\4_word_txt\"  # Folder to save word-specific TXT files\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "# List of target words\n",
    "words = [\"bat\", \"cup\", \"drop\", \"eat\", \"fish\", \"hot\", \"jump\", \"milk\", \"pen\", \"red\"]\n",
    "\n",
    "# Iterate through all processed CSV files\n",
    "for file_name in os.listdir(processed_folder):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_path = os.path.join(processed_folder, file_name)\n",
    "        data = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "\n",
    "        # Ensure the CSV has the required structure\n",
    "        if len(data.columns) < 2:\n",
    "            print(f\"File {file_name} does not have enough columns. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Extract the Word and Sentence/Question columns\n",
    "        word_column = data.columns[0]  # First column is Word\n",
    "        sentence_column = data.columns[1]  # Second column is Sentence/Question\n",
    "\n",
    "        # Process each word and write to its respective TXT\n",
    "        for word in words:\n",
    "            # Filter rows where the Word column matches the current word\n",
    "            word_rows = data[data[word_column] == word]\n",
    "\n",
    "            if not word_rows.empty:\n",
    "                # Prepare the content for the word TXT\n",
    "                content = [sentence.strip() for sentence in word_rows[sentence_column]]\n",
    "\n",
    "                # Write to the word-specific TXT file without extra spaces\n",
    "                output_file = os.path.join(output_folder, f\"{word}.txt\")\n",
    "                with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(\"\\n\".join(content) + \"\\n\")  # Single newline between sentences\n",
    "\n",
    "print(f\"Processing completed. Word-specific files are saved in {output_folder}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T20:40:35.122805200Z",
     "start_time": "2024-12-01T20:40:34.384755100Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
